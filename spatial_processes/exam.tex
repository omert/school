\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}
 
\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}



% \newcommand{\enote}[1]{{\bf [[Elchanan:} {\emph{#1}}{\bf ]]}}
% \newcommand{\knote}[1]{{\bf [[Krzysztof:} {\emph{#1}}{\bf ]]}}
% \newcommand{\rnote}[1]{{\bf [[Ryan:} {\emph{#1}}{\bf ]]}}



\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Random Spatial Processes - Homework}

 \author{Omer Tamuz, 035696574}
\maketitle


\begin{enumerate}
  \item
  \begin{enumerate}
  \item Let $G_X(\eta)=\E[\eta^X]$ where $X$ is distributed geometrically with
    parameter $p$. We showed in class that when $p \leq \half$ then the 
    extinction probability is one,
    since the expcted number of offsprings is less than or equal to one.
    For $p>\half$ we showed that the extinction probability $\eta$ is
    the smallest solution of $\eta=G_X(\eta)$:
    \begin{eqnarray*}
      \eta &=& G_X(\eta)
      \\ &=& \E[\eta^X]
      \\ &=& \sum_{i=0}^\infty \eta^i\P[X=i]
      \\ &=& \sum_{i=0}^\infty \eta^i(1-p)p^i
      \\ &=& (1-p)\sum_{i=0}^\infty (\eta p)^i
      \\ &=& {1-p \over 1 - \eta p}
    \end{eqnarray*}
    This gives a quadratic equation on $\eta$: $p\eta^2-\eta+1-p=0$.
    \begin{eqnarray*}
      0 &=& p\eta^2-\eta+1-p
      \\ 0 &=& \eta^2-\eta/p+{1-p \over p}
      \\ 0 &=& (\eta-1)(\eta-(1/p-1)).
    \end{eqnarray*}
    Since $p>\half$ then $\eta=1/p-1$.
  \item
    In the paper refered to in the question it is shown that a branching
    process's geneological tree contains an infinite binary tree with
    probability $\pi$ where $1-\pi$ is the smallest root in $[0,1]$ of
    $$G_X(s)+(1-s)G'_X(s)=s.$$
    In our case $G_X(s)=(1-p)/(1-sp)$ and $G'_X(s)=p(1-p)/(1-sp)^2$ 
    and so $1-\pi_p$ is the smallest root in $[0,1]$ of
    \begin{eqnarray*}
      {1-p \over 1-sp}+{(1-s)p(1-p)\over (1-sp)^2}=s
    \end{eqnarray*}
    The solutions to this equation are $s_0=1$,
    $$s_1={1\over p}-{1\over 2}-{1\over 2}\sqrt{5-4/p}$$
    and
    $$s_2={1\over p}-{1\over 2}+{1\over 2}\sqrt{5-4/p}.$$
    
    The following results are a consequence of simple analysis, which we
    omit:

    When $p<4/5$ then $s_1$ and $s_2$ are complex and so $\pi_p=1-s_0=0$ and no
    infinite binary tree exists.

    When $p=4/5$ then $s_1=s_2=5/4-1/2=3/4$ and so we have a phase transition,
    and a binary tree exists with probability $\pi_p=1-s_1=1/4$.

    When $4/5<p<1$ then $0<s_1<3/4<s_2$ and so $\pi_p=1-s_1$.

    When $p=1$ then $s_1=0$ and $\pi_p=1-s_0=1$.
  \item
    Let $\theta_m$ be the probability that the geneological tree of a 
    branching process with Binomial($n,p$) offspring distribution 
    has an $(n-1)$-ary tree of depth $m$, rooted at the time 
    zero individual. This happens if this individual has $n-1$ or $n$
    offsprings and if at least $n-1$ of these offsprings have such trees of 
    depth $m-1$:
    $$\theta_m=p_{n-1}\theta_{m-1}^{n-1}+p_n\left[\theta_{m-1}^n+
      \theta_{m-1}^{n-1}(1-\theta_{m-1})n\right]$$
    The probability $p_n$ for $n$ offsprings is $p^n$. The probability $p_{n-1}$ 
    for $n-1$ offsprings is $np^{n-1}(1-p)$.
    Hence $\theta$, the probability of an infinite $(n-1)$-ary tree, is a root
    of the map
    $$f_p(x)=np^{n-1}(1-p)x^{n-1}+p^n\left[ x^n+x^{n-1}(1-x)n \right]-x.$$
    This is clearly a continuous function, and so we shall show that it has a 
    root in $(0,1)$ by showing that it takes both negative and positive values
    in this range:

    $f_p(0)=0$ and $\lim_{x\to 0}f_p(x)/x=-1$
    so for $\epsilon>0$ small enough $f_p(\epsilon)/\epsilon<0$ and in particular 
    $f_p(\epsilon)<0$. On the other hand, at $p=1$, $x=1$ we have $f_1(1)=0$ and 
    $f_1'(1)=-1$, and so since $f_p(x)$ is continuous in $x$ there exists an 
    $\epsilon$ such that $f_1(1-\epsilon)>0$. Since $f_p(x)$ is also continuous
    in $p$ then there exists a $\delta$ such that $f_{1-\delta}(1-\epsilon)>0$.
  \end{enumerate}
  \item
    \begin{enumerate}
    \item 
      Add fiducial $B_{1,1}^0$ ($=A_0$) and $X_{1,1}^0$ distributed as $X_{i,j}^n$
      above. 
      Consider $B_{i,j}^n$ an offspring of $B_{k,l}^{n-1}$ if the latter contains
      the former. Then this problem is a representation of a branching process,
      with offspring
      distribution $B(9,p)$ (and conditioned on $X_{1,1}^0=1$). 
      The event $A_0\neq \emptyset$ is equivalent to
      the branching process not dying out, and therefore has positive
      probability when $p>1/9$.
      
    \item
      \begin{claim}
        If the branching process contains an 8-ary infinite tree rooted
        at $B_{1,1}^0$ then $A_\infty$ has a left-to-right crossing.
      \end{claim}
      Note that by 1(c) this statement implies that $p_c<1$, since there exists
      a $q<1$ such that there is a positive probability for an 8-are tree,
      and by the above $p_c<q$.

      \begin{proof}
        Let the branching process contain an 8-ary infinite tree rooted at 
        $B_{1,1}^0$. We define a series of function $f_n:[0,1]\to[0,1]^2$.
        Let $f_1$ be a function with $f_1(0)=(0,y_0)$ and 
        $f_1(1)=(0,y_1)$ for some $y_0,y_1$, and $f_1$ only passes through
        $B_{i,j}^1$'s that are part of the 8-ary binary tree rooted at the root.
        Since there are at least eight such $B_{i,j}^1$'s, it is easy to see 
        that such an $f_1$ exists.

        We define $f_n$ recursively to only pass through the same $B_{i,j}^{n-1}$'s
        that $f_{n-1}$ passed through, and in addition only pass through
        the $B_{i,j}^n$'s that are
        part of the 8-ary tree rooted at $B_{1,1}^0$. This can be done by slightly 
        distorting the image of $f_{n-1}$:

        Let $C$ and $D$ be
        two neighboring 
        $B_{i,j}^{n-1}$'s such that $f_{n-1}$ crosses from $C$ to $D$. Since they each have at 
        least eight offsprings that are included in the tree, then $C$ and $D$
        have adjacent offsprings $C'$ and $D'$ (in the sense that they share an 
        edge).
        Since $f_{n-1}$ crosses from $C$ to $D$, then if necessary we distort $f_n$ so 
        that it will cross from
        $C'$ to $D'$. We perfrom this distortion while keeping constant the $x$
        in which the function crosses from $C$ to $D$.
        
        Having made sure that $f_n$ crosses from $C$ to $D$ in legal offsprings,
        we may need to distort it some more so that {\em while in} $C$ it 
        passes only
        through offsprings belonging to the tree. It is easy to see that this
        can also be done, again because $C$ has at least eight different 
        offsprings, and so any two points on the edge of $C$ that belong to
        offsprings can be connected through offsprings.

        Finally, we may again need to shift $f_n$ slightly so that its starting
        and end points are in the tree. This can also always be done (again
        because we have eight offsprings) within
        the $B_{i,j}^{n-1}$ that $f_{n-1}$ started (or ended) at, while keeping the condition
        that $f_n(0)\in\{0\}\times [0,1]$ and $f_n(1)\in\{1\}\times [0,1]$.
        
        We've shown that the image of $f_n$ is in $A_n$. Furthermore, 
        since all the distortions we performed are within boxes of dimensions
        $3^{-n}\times 3^{-n}$, then the $L_\infty$ distance between $f_{n-1}$ and
        $f_n$ is bounded by $O(3^{-n})$, and the series $f_n$ converges uniformly. 
        
        Let $f_\infty$ be the limit of $f_n$. Then we claim that $f_\infty$ is
        a left to right crossing:
        \begin{itemize}
        \item Since $f_n$ is a series of continuous functions that converges
          uniformly then $f_\infty$ is also continuous.
        \item Since the image of $f_n\in A_n$, and since the $A_n$'s are closed
          then the image of $f_\infty$ is in $A_n$ for all $n$. Hence the image
          of $f_\infty$ is in $A_\infty$.
        \item Since  $f_n(0)\in\{0\}\times [0,1]$ and 
          $f_n(1)\in\{1\}\times [0,1]$ for all $n$ then this also holds for
          $f_\infty$.
        \end{itemize}
        
        We have thus shown that the existence of an 8-ary tree implies the
        existence of a left-to-right crossing.
      \end{proof}
    \end{enumerate}
  \item
  \item
    \begin{enumerate}
    \item 
      Let $T_k$ be the amount of time it takes the random walk to cover
      $k+1$ vertices after having covered $k$ vertices. Then clearly
      $C_n=\sum_{k=1}^{n-1}T_n$ (we start from $k=1$ since at time zero the
      walk have already covered one vertex).
      
      $T_k$ is distributed geometrically with parameter 
      $p_k=(n-k)/(n-1)$: The
      probability of adding a new vertex to the set of those covered, after 
      having already covered $k$ vertices, is the probability of choosing
      one of the $n-k$ unvisited vertices out of the $n-1$ possible ones. 
      Therefore
      \begin{eqnarray*}
        \E[C_n]&=&\E\left[\sum_{k=1}^{n-1}T_k\right]
        \\ &=&\sum_{k=1}^{n-1}\E[T_k]
        \\ &=&\sum_{k=1}^{n-1}{1\over p_k}
        \\ &=&\sum_{k=1}^{n-1}{n-1\over (n-k)}
        \\ &=&(n-1)\sum_{k=1}^{n-1}{1\over (n-k)}
        \\ &=& (n-1)\left({1\over n-1}+{1\over n-2}+\cdots+1\right)
      \end{eqnarray*}
    \item
      Let $P_k^n$ be the probability that vertex $x_0$ was not visited until
      time $k$. Then $P_0=(n-1)/n$ and for $k>0$ we have that 
      $P_k^n=P_{k-1}^n(n-2)/(n-1)$, since if $x_0$ was not visited up to time 
      $P_{k-1}$ then in particular $X_{k-1}\neq x_0$ and the probability to visit
      it at time $k$ is $(n-2)/(n-1)$. We can therefore deduce that
      $$P_k^n={n-1\over n}\left({n-2\over n-1}\right)^k.$$
      We use this in the  following calculation:
      \begin{eqnarray*}
        {\E[D_{[un]}]\over n} &=& {1\over n}\E\left[\sum_{x\in K_n}{\bf 1}_{x \not \in \{X_0,\ldots,X_{[un]}\}}\right]
        \\ &=& {1\over n}\sum_{x\in K_n}\E\left[{\bf 1}_{x \not \in \{X_0,\ldots,X_{[un]}\}}\right]
        \\ &=& {1\over n}n\P[x_0 \not \in \{X_0,\ldots,X_{[un]}\}]
        \\ &=& P_{[un]}^n
        \\ &=& {n-1\over n}\left({n-2\over n-1}\right)^{[un]}.
        \end{eqnarray*}
        Then
        \begin{eqnarray*}
          {n-1\over n}\left({n-2\over n-1}\right)^{un+1} &\leq {\E[D_{[un]}]\over n} \leq& {n-1\over n}\left({n-2\over n-1}\right)^{un-1}
          \\ \lim_{n \to \infty}{n-1\over n}\left({n-2\over n-1}\right)^{un+1} &\leq \lim_{n \to \infty}{\E[D_{[un]}]\over n} \leq& \lim_{n \to \infty}{n-1\over n}\left({n-2\over n-1}\right)^{un-1}
          \\ \lim_{n \to \infty}{n\over n+1}\left({n-1\over n}\right)^{u(n+1)+1} &\leq \lim_{n \to \infty}{\E[D_{[un]}]\over n} \leq& \lim_{n \to \infty}{n\over n+1}\left({n-1\over n}\right)^{u(n+1)-1}
          \\ \lim_{n \to \infty}{n\over n+1}\left(1-{1\over n}\right)^{n(u+u/n+1/n)} &\leq \lim_{n \to \infty}{\E[D_{[un]}]\over n} \leq& \lim_{n \to \infty}{n\over n+1}\left(1-{1\over n}\right)^{n(u+u/n-1/n)}
          \\ \lim_{n \to \infty}\exp(-u-u/n-1/n) &\leq \lim_{n \to \infty}{\E[D_{[un]}]\over n} \leq& \lim_{n \to \infty}\exp(-u-u/n+1/n)
          \\ \exp(-u) &\leq \lim_{n \to \infty}{\E[D_{[un]}]\over n} \leq& \exp(-u)
        \end{eqnarray*}
      \item
        By calculations nearly identical to the one above, we can show that
        \begin{equation}
          \label{eq:plus_eps}
          \lim_{n\to\infty}{\E[D_{n\log n(1+\epsilon)}]\over n^{-\epsilon}}=1
        \end{equation}
        and
        \begin{equation}
          \label{eq:minus_eps}
          \lim_{n\to\infty}{\E[D_{n\log n(1-\epsilon)}]\over n^{\epsilon}}=1.
        \end{equation}
        
        Since~\eqref{eq:plus_eps} implies that $\E[D_{n\log n(1+\epsilon)}]$
        goes to zero as $n$ goes to infinity, then by the Markov
        inequality $\P[D_{n\log n(1+\epsilon)}\geq 1]$ also
        goes to zero. Hence with probability that approaches one, after
        $n\log n(1+\epsilon)$ steps all the vertices are covered. In terms
        of $C_n$ this means that
        $\P[C_n/(n\log n)<1+\epsilon]\to 1$.

        The probability $Q_k^n$ that two vertices, $x_0$ and $x_1$, were not 
        visited until time $k$ is, by analysis similar to the one above for
        $P_k^n$, equal to 
        \begin{eqnarray*}
          Q_k^n&=&{n-2\over n}\left({n-3\over n-1}\right)^k.
          \\ &=& P_k^n{n-2\over n-1}\left({n-3\over n-2}\right)^k
          \\ &=& P_k^nP_k^{n-1}
        \end{eqnarray*}
        We can use this to calculate the variance of $D_k^n$ 
        ($=D_k$ for $n$ vertices):
        \begin{eqnarray*}
          \\Var[D_k^n]&=&\E[{D_k^n}^2]-\E[D_k^n]^2
          \\ &=& \E\left[\sum_{x,y\in K_n}{\bf 1}_{x \not \in \{X_0,\ldots,X_k\}}{\bf 1}_{y \not \in \{X_0,\ldots,X_k\}}\right]-\E[D_k^n]^2
          \\ &=& \E\left[\sum_{x\in K_n}{\bf 1}_{x \not \in \{X_0,\ldots,X_k\}}^2+\sum_{x\neq y\in K_n}{\bf 1}_{x \not \in \{X_0,\ldots,X_k\}}{\bf 1}_{y \not \in \{X_0,\ldots,X_k\}}\right]-\E[D_k^n]^2
          \\ &=& \E[D_k^n]+2{n \choose 2}\P\left[{\bf 1}_{x_0 \not \in \{X_0,\ldots,X_k\}}{\bf 1}_{x_1 \not \in \{X_0,\ldots,X_k\}}\right]-\E[D_k^n]^2
          \\ &=& \E[D_k^n]+n(n-1)P_k^nP_k^{n-1}-\E[D_k^n]^2
          \\ &=& \E[D_k^n]\left(1+\E[D_k^{n-1}]-\E[D_k^n]\right)
        \end{eqnarray*}
        Now $\E[D_k^n]$ is clearly monotonically decreasing in $n$, and 
        therefore $\Var[D_n^k]\leq \E[D_k^n]$.
        
        By~\eqref{eq:minus_eps}, $\E[D_{n\log n(1-\epsilon)}]$ is larger than 
        $0.9n^\epsilon$ for $n$ large enough. Hence by Chebychev's inequality
        (for example), the probability that $D_{n\log n(1-\epsilon)}$ is zero
        goes to zero as $n$ diverges. In terms of $C_n$ this means that
        $\P[C_n/(n\log n)>1-\epsilon]\to 1$.

        We have therefore shown that
        $$\lim_{n\to\infty}\P\left[{C_n\over n\log n}\in (1-\epsilon,1+\epsilon)\right]=1$$
        for any $\epsilon>0$.
    \end{enumerate}

\item
  Interestingly, this process behave differently for even or odd $n$. For
  odd $n$, the initial degrees of the verices are even. Hence, except for 
  the initial vertex, every vertex that the process enters it can also leave.
  Therefore the process gets stuck at the same vertex from which it started.
  When $n$ is even, then by the same argument the process cannot end at the
  same vertex at which it started, but gets stuck at some other vertex.
  
  The implication of the above is that when $n$ is odd there may be many
  vertices with degree zero at the time that the process gets stuck. When $n$
  is even there may be only one such vertex (the one at which the process
  started).
  
  Unfortunately, we have no further rigorous results for this open problem. 
  Some 
  intuition can be found, however, in assuming that the process mixes quickly
  up until it gets stuck. Whereas this approximation is clearly not very good
  towards the end of the walk, it seems adequate at its beginning and 
  conceivably also up until much later. 

  Under this ``quick-mixing approximation'', 
  the process is equivalent to picking and removing an edge at random at
  each iteration. Let $t=n(n-1)/2$. Label the edges $e_1,\ldots,e_T$, 
  and assume they are removed according to this order. 

  Let $v_0$ be the vertex the process starts at. Consider the case of odd $n$, 
  in which the process ends once $v_0$ has degree zero. Then the process ends
  at time $S_n$ where $S_n=\max\{t|e_t\mbox{ belongs to } v_0\}$. The probability of a particular
  edge to be labeled $e_s$ where $s\leq t$ is $t/T$. Given that one such edge was thus
  labeled then the probability that another edges of $v_o$ is labeled $\leq t$
  is $(t-1)/(T-1)$. Hence the probability
  that all $n-1$ edges of $v_0$ are labeled $\leq t$, and so the process finishes
  up to time $t$ is 
  \begin{equation*}
    \P[S_n\leq t] = \prod_{i=1}^{n-1}{t-i+1\over T-i + 1},
  \end{equation*}
  for $t\geq n-1$. Hence the expected value of $S_n$ is
  \begin{eqnarray*}
    \E[S_n]&=& \sum_{t=1}^T\left(1-\P[S_n\leq t]\right)
    \\ &=&n-2+\sum_{t=n-1}^T\left(1-\prod_{i=1}^{n-1}{t-i+1\over T-i + 1}\right)
    \\ &=& T - \sum_{t=n-1}^T\prod_{i=1}^{n-1}{t-i+1\over T-i + 1}
  \end{eqnarray*}
  We can bound $\E[S_n]$ by noting that $(t-i+1)/(T-i+1)<t/T$ for $i\geq 1$:
  \begin{eqnarray*}
    \E[S_n] &\geq&T - \sum_{t=n-1}^T\prod_{i=1}^{n-1}{t\over T}
    \\ &=& T - \sum_{t=n-1}^T\left({t\over T}\right)^{n-1}
    \\ &=& T - T^{-(n-1)}\sum_{t=n-1}^Tt^{n-1}
    \\ &\geq& T - T^{-(n-1)}{T^n \over n}
    \\ &=& T-{n-1\over 2}.
  \end{eqnarray*}
  
  Therefore, according to this approximation, we expect $(n-1)/2$ edges to be 
  left at the end of the process. 
 
  Simulations seem to agree with the validity of these approximations. We ran
  10,000 simulations of this process on a graph with 201 vertices. The average
  number of vertices left was 95, whereas the analysis above predicts 100.

\end{enumerate}
\end{document}


  \item
    \begin{enumerate}
    \item 
      Let $G(n,p)$ be a random E-R graph with $p={\log n-a_n\over n}$ as defined
      in the question. Let $X=\sum_{v\in[n]}{\bf 1}_{C(v)=\{v\}}$, also as defined
      in the question. Then the expectation of $X$ is:
      \begin{eqnarray*}
        \E[X] &=& \E\left[\sum_{v\in[n]}{\bf 1}_{C(v)=\{v\}}\right]
        \\ &=& n\P[C(v_0)=\{v_0\}]
        \\ &=& n\P[C(v_0)=\{v_0\}]
        \\ &=& n(1-p)^{n-1}.
        \\ &=& n\left(1-{\log n-a_n\over n}\right)^{n-1}.
      \end{eqnarray*}
      The limit of $\E[X]$ as $n$ tends to infinity is 
      $n\exp(a_n-\log n)=\exp(a_n)=\infty$.
      The expectation of $X^2$ is:
      \begin{eqnarray*}
        \E[X^2] &=& \E\left[\left(\sum_{v\in[n]}{\bf 1}_{C(v)=\{v\}}\right)^2\right]
        \\ &=& \E\left[\sum_{u,v\in[n]}{\bf 1}_{C(v)=\{v\}}{\bf 1}_{C(u)=\{u\}}\right]
        \\ &=& \sum_{u,v\in[n]}\P[C(v)=\{v\} \mbox{ and } C(u)=\{u\}]
        \\ &=& \sum_{u\neq v\in[n]}\P[C(v)=\{v\} \mbox{ and } C(u)=\{u\}]+\sum_{v\in[n]}\P[C(v)=\{v\}]
        \\ &=& n(n-1)\P[C(v)=\{v\} \mbox{ and } C(u)=\{u\}]+\E[X]
        \\ &=& n(n-1)(1-p)^{2(n-1)-1}+\E[X]
        \\ &=& {n-1\over n(1-p)}\E[X]^2+\E[X]
      \end{eqnarray*}
      Hence
      \begin{eqnarray*}
        \Var[X]&=&\E[X^2]-\E[X]^2
        \\ &=& {n-1\over n(1-p)}\E[X]^2+\E[X] - \E[X]^2
        \\ &=& {np-1\over n-np}\E[X]^2+\E[X] 
      \end{eqnarray*}
      We can now bound the probability that the graph is not connected:
      \begin{eqnarray*}
        \P[\mbox{graph not connected}] &\geq& 
        \P[\mbox{there exists an isolated vertex}]
        \\ &=& \P[X>0]
        \\ &\geq& \P\left[|X-\E[X]| < \E[X]/2\right]
        \\ &\geq& 1-{\Var[X] \over \E[X]^2/4}
        \\ &=& 1-{4(np-1)\over n-np}+{4 \over \E[X]}
        \\ &=& 1-{4(\log n-a_n-1)\over n-\log n + a_n}+{4 \over \E[X]}.
      \end{eqnarray*}
      Since $a_n$ is bounded by $\log n$ then the second term vanishes as
      $n$ tends to infinity, as does the last term. Hence the probability
      that the graph is connected tends to zero as $n$ tends to infinity.
    \end{enumerate}
