\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{amsthm} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}

\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{proposition*}{Proposition}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}


\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}
\newcommand{\fifth}{{\textstyle \frac15}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Combinatorial Statistics - Homework Set 2 Solution}

\date{\today}
\maketitle
\subsection{Markov Random Fields}
{\bf The Markov Property}

Assume the graph is connected. If not, clearly each connected component is
independent and apply the proof below to each component.
Given disjoint $A_0$, $B_0$ and $S$ such that $S$ separates $A_0$ and $B_0$,
we ``grow'' $A_0$ by repeatedly adding to it any vertex that neighbors it
and is not in $S$ or $A_0$. We name it $A$ in its ``adult'' state. We do
the same for $B_0$, creating $B$. It is easy to
convince oneself that $S$ separates $A$ and $B$, and that the complement of the 
union of $A$, $B$ and $S$ does not neighbor $A$ or $B$. We call this complement
$R$.

Given some $x_S$, we denote by $f_C$ the function $\psi_C$ restricted
to $\sigma_S=x_S$. Note that $f_C$ is constant when $C\subseteq S$. Let the notation $C\sim A$ 
signify the same as $C\cap A\neq\emptyset$. Then one and only one of $C\sim A$, $C\sim B$,
$C\sim R$, $C\subseteq S$  holds for a given clique $C$.

Now,
\begin{eqnarray*}
  \lefteqn{\P[\sigma_S=x_S]}
\\ &=& Z^{-1}\sum_{y_{A\cup B\cup R}}\prod_Cf_C(y_{A\cup B})
\\ &=& Z^{-1}\sum_{y_{A\cup B\cup R}}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(y_A)\prod_{C\sim B}f_C(y_B)\prod_{C\sim R}f_C(y_R)
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\sum_{y_A}\prod_{C\sim A}f_C(y_A)\sum_{z_B}\prod_{C\sim B}f_C(z_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Likewise
\begin{eqnarray*}
  \lefteqn{\P[\sigma_A=x_A,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(x_A)\sum_{z_B}\prod_{C\sim B}f_C(z_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
and
\begin{eqnarray*}
  \lefteqn{\P[\sigma_B=x_B,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim B}f_C(x_B)\sum_{z_A}\prod_{C\sim A}f_C(z_A)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Finally,
\begin{eqnarray*}
  \lefteqn{\P[\sigma_A=x_A,\sigma_B=x_B,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(x_A)\prod_{C\sim B}f_C(x_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Hence
\begin{eqnarray*}
\lefteqn{\P[\sigma_{A\cup B}=x_{A\cup B}|\sigma_S=x_S]\over  
    \P[\sigma_A=x_A|\sigma_S=x_S]\P[\sigma_B=x_B|\sigma_S=x_S]}
\\  &=&   {\P[\sigma_{A\cup B}=x_{A\cup B},\sigma_S=x_S]\P[\sigma_S=x_S]\over  
    \P[\sigma_A=x_A,\sigma_S=x_S]\P[\sigma_B=x_B,\sigma_S=x_S]}
\end{eqnarray*}
We have shown that $\sigma_A$ and $\sigma_B$ are independent when conditioned
on $\sigma_S$. This implies that the same holds for $A_0$ and $B_0$:
\begin{eqnarray*}
\lefteqn{\P[\sigma_{A_0\cup B_0}=x_{A_0\cup B_0}|\sigma_S=x_S]}
\\ &=& 
\sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_{A_0\cup B_0}=x_{A_0\cup B_0},\sigma_{A\setminus A_0}=x_{A\setminus A_0},\sigma_{B\setminus B_0}=x_{B\setminus B_0}|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_{A\cup B}=x_{A\cup B}|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_A=x_A|\sigma_S=x_S]\P[\sigma_B=x_B|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\P[\sigma_A=x_A|\sigma_S=x_S]\sum_{x_{B\setminus B_0}}\P[\sigma_B=x_B|\sigma_S=x_S]
\\ &=& \P[\sigma_{A_0}=x_{A_0}|\sigma_S=x_S]\P[\sigma_{B_0}=x_{B_0}|\sigma_S=x_S]
\end{eqnarray*}


{\bf Max-Cuts}

Let max-cuts of $G$ have $k$ edges crossing the cut. Then, if $\tau$ is a particular
max-cut, then
\begin{eqnarray*}
  \P[\tau]&=&Z^{-1}\exp\left(100nk-100n(|E|-k)\right)
  \\ &=& Z^{-1}\exp\left(200nk-100n|E|\right)
\end{eqnarray*}
The probability of a cut $\sigma$ that is not maximal is at most
\begin{eqnarray*}
  \P[\sigma]&=&Z^{-1}\exp\left(100n(k-1)-100n(|E|-(k-1))\right)
  \\ &=& \P[\tau]\exp\left(-200n\right)
\end{eqnarray*}
Since there are less than $2^n=\exp(n\log 2)$ cuts that are not maximal, then
\begin{eqnarray*}
  \P[\sigma\mbox{ is not a MAX-CUT of $G$}] &\leq& \P[\tau]\exp\left(-n(200-\log 2)\right)
\\ &\leq& \exp(-n(200-\log 2)) 
\\ &\leq& \exp(-199) 
\end{eqnarray*}

\subsection{The Transportation Distance and Total Variation}
{\bf Transportation Distance}

\begin{itemize}
\item To show this we need to prove the triangle inequality and that
  $d_D(P_1,P_2)=0 \leftrightarrow P_1=P_2$; positivity and symmetry follow immediately from
  the positivity and symmetry of $D$.
  \begin{enumerate}
  \item Given $X\sim P_X$, $Y\sim P_Y$ and $Z\sim P_Z$, let $P_{XY}$ be
    a coupling that minimizes $\E(D(X,Y))$ and $P_{YZ}$ a coupling that
    minimizes $\E(D(Y,Z))$. Define $P_{XZ}$ by:
    $$\P_{XZ}[X=x,Z=z]=\sum_y{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}.$$ 
    Now 
    \begin{eqnarray*}
      \P_{XZ}[X=x]&=&\sum_z\P_{XZ}[X=x,Z=z]
      \\ &=& \sum_{y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}
      \\ &=& \sum_{y}{\P_{XY}[X=x,Y=y]\P_{Y}[Y=y]\over \P_Y[Y=y]}
      \\ &=& \sum_{y}\P_{XY}[X=x,Y=y]
      \\ &=& \P_X[X=x]
    \end{eqnarray*}
    and likewise $\P_{XYZ}[Z=z]=\P_Z[Z=z]$. By corollary, $P_{XZ}$ is a
    distribution, and a coupling between $X$ and $Z$. Hence 
    \begin{eqnarray*}
      d_D(P_X,P_Z)&\leq& \E_{P_{XZ}}(D(X,Z)) 
      \\ &=& \sum_{x,z}\P_{XZ}[X=x,Z=z]D(X,Z)
      \\ &=& \sum_{x,y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}D(X,Z)
      \\ &\leq&\sum_{x,y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}(D(X,Y)+D(Y,Z))
      \\ &=&\sum_{x,y}\P_{XY}[X=x,Y=y]D(X,Y)+\sum_{y,z}\P_{YZ}[Y=y,Z=z]D(Y,Z)
      \\ &=&d_D(P_X,P_Y)+d_D(P_Y,P_Z)
    \end{eqnarray*}
    
  \item If $P_1=P_2$ then by the trivial coupling ($X$ and $Y$ are identical)
    we have $\E(D(X,Y))=0$. 

    Assume $d_D(P_1,P_2)=0$. Then there exists a coupling $P$ such that  
    $(X,Y)\sim P$,
    $X\sim P_1$, $Y\sim P_2$ and $\E_P(D(X,Y))=0$. Hence $\P[X\neq Y]=0$ and
    $\P[X=\omega]=P[Y=\omega]$ and $X$ and $Y$ are distributed identically.
  \end{enumerate}
  \item 
    For  $X\sim P_1$, $Y\sim P_2$ and $Z\sim P_3$, let $P_{13}$ and $P_{23}$ 
    be couplings that minimize $\E(D(X,Z))$ and
    $\E(D(Y,Z))$.
    
    Define $W$ to equal $X$ with probability $\alpha$ and $Y$ with probability
    $1-\alpha$, so that $W$'s set of possible outcomes is the union of those of
    $X$ and $Y$. Then $W\sim P_\alpha$ with  $P_\alpha=\alpha P_1 +(1-\alpha)P_2$. We can likewise define a 
    coupling between $W$ and $Z$: with probability $\alpha$ couple $Z$ with
    $X$ using $P_{13}$, and with probability $1-\alpha$ couple $Z$ with $Y$
    using $P_{23}$. The joint coupled probability distribution of $W$ and $Z$
    is hence $P_{\alpha 3}=\alpha P_{13} +(1-\alpha)P_{23}$. Then
    \begin{eqnarray*}
      \P_{\alpha 3}[W=w]&=&\alpha \P_{13}[W=w] +(1-\alpha)\P_{23}[W=w]
      \\ &=& \alpha \P_1[W=w]+(1-\alpha)\P_2[W=w]
      \\ &=& \P_\alpha[W=w]
    \end{eqnarray*}
    and
    \begin{eqnarray*}
      \P_{\alpha 3}[Z=z]&=&\alpha \P_{13}[Z=z] +(1-\alpha)\P_{23}[Z=z]
      \\ &=& \alpha \P_3[Z=z]+(1-\alpha)\P_3[Z=z]
      \\ &=& \P_3[Z=z]
    \end{eqnarray*}
    and so $P_{\alpha 3}$ is indeed a coupling of $P_\alpha$ and $P_3$. Finally
    \begin{eqnarray*}
      \lefteqn{d_D(\alpha P_1+(1-\alpha)P_2,P_3)}
      \\ &=& d_D(P_\alpha,P_3)
      \\ &\leq&  \E_{P_{\alpha 3}}D(W,Z)
      \\ &=&  \sum_{w,z}\P_{\alpha 3}[W=w,Z=z]D(W,Z)
      \\ &=& \sum_{w,z}\left(\alpha \P_{13}[W=w,Z=z] +(1-\alpha)\P_{23}[W=w,Z=z]\right)D(W,Z)
      \\ &=& \alpha \sum_{x,z}\P_{13}[W=x,Z=z]D(W,Z) +(1-\alpha)\sum_{y,z}\P_{23}[W=y,Z=z]D(W,Z)
      \\ &=& \alpha d_D(P_1,P_3)+(1-\alpha)d_D(P_2,P_3)
    \end{eqnarray*}
    \item The previous two items imply that the total variation distance is a 
      convex metric.
\end{itemize}

{\bf Total Variation}

Again let $X\sim P_1$ and $Y\sim P_2$ over distribution space $\Omega$. 
It was demonstrated in class that $d_{TV}(P_1,P_2)\geq \min_P P[X\neq Y]$, where $P$ is any
coupling between $P_1$ and $P_2$. 
We need then only
show that $d_{TV}(P_1,P_2)\leq \min_P P[X\neq Y]$. Let $\Omega_1 \subseteq \Omega$ be those
values of $\omega$ for which $\P_1(\omega)\geq P_2(\omega)$, and define 
$\Omega_2$ to be its complement. Then:
\begin{eqnarray*}
  d_{TV}(P_1,P_2)&=&{1\over 2}\sum_{\omega \in \Omega}|P_1(\omega)-P_2(\omega)|
  \\ &=& {1\over 2}\sum_{\omega \in \Omega}\max\{P_1(\omega),P_2(\omega)\}
  - \min\{P_1(\omega),P_2(\omega)\}
  \\ &=& {1\over 2}\left(\sum_{\omega\in\Omega_1}P_1(\omega)
  + \sum_{\omega\in\Omega_2}P_2(\omega)\right)
  - {1\over 2}\sum_{\omega \in \Omega}\min\{P_1(\omega),P_2(\omega)\}
  \\ &=& {1\over 2}\left(2-\sum_{\omega\in\Omega_2}P_1(\omega)
  - \sum_{\omega\in\Omega_1}P_2(\omega)\right)
  - {1\over 2}\sum_{\omega \in \Omega}\min\{P_1(\omega),P_2(\omega)\}
  \\ &=& {1\over 2}\left(2-
    \sum_{\omega \in \Omega}\min\{P_1(\omega),P_2(\omega)\}  \right)
  - {1\over 2}\sum_{\omega \in \Omega}\min\{P_1(\omega),P_2(\omega)\}
  \\ &=& 1- \sum_{\omega \in \Omega}\min\{P_1(\omega),P_2(\omega)\}.
\end{eqnarray*}
Now let $P$ be a coupling between $P_1$ and $P_2$. Then for any $x,y$ it holds
that $P(x,y)\leq P_1(x)$, since $P_1(x)=\sum_yP(x,y)$. 
Likewise, $P(x,y)\leq P_2(y)$, and so $P(x,y)\leq \min\{P_1(x,x),P_2(y,y)\}$. Hence:

\begin{eqnarray*}
\P[X\neq Y]&=&1-\P[X=Y]
\\ &=& 1-\sum_x P(x,x)  
\\ &\geq&1-\sum_x \min\{P_1(x),P_2(x)\}
\\ &=& d_{TV}(P_1,P_2).
\end{eqnarray*}
We have shown that  $d_{TV}(P_1,P_2)\leq \P[X\neq Y]$ for any coupling $P$,
and so $d_{TV}(P_1,P_2)\leq \min_P\P[X\neq Y]$


{\bf An Example}

Denote by $x$ a vector of $n$ bits: $x=\{x_i\}=(x_1,\ldots,x_n)$. For a
random variable $X$ taking values of type $x$, denote by
$X_i$ the $i$'th bit of $X$.

We'll solve for general $p$ and $q$, assuming only that $q>p$.
\begin{itemize}
\item {\bf Total Variation}
For $P_q^1$ and $P_q^1$ the calculation is straightforward:
\begin{eqnarray*}
  d_{TV}(P_q^1,P_p^1)&=&{1\over 2}((q-p)+((1-p)-(1-q)))
  \\ &=& q-p
\end{eqnarray*}
For $P_q^3$ and $P_p^3$ we note that since $q$ is greater than $p$ then
$P_q^3(x)>P_p^3(x)$ whenever $x$ has more ones than zeros. Hence:

\begin{eqnarray*}
  d_{TV}(P_q^3,P_p^3)&=&1-\sum_{x}\min\left\{P_q^3(x),P_p^3(x)\right\}
\\ &=& 1-\sum_{\sum_ix_i>1}P_p^3(x)-\sum_{\sum_ix_i\leq 1}P_q^3(x)
\\ &=& 1-p^3-3p^2(1-p)-(1-q)^3-3(1-q)^2q
\\ &=& 1-p^2(3-2p)-(1-q)^2(1+2q)
\\ &=& 2(p^3-q^3)+3(q^2-p^2)
\end{eqnarray*}

For $q=0.8$ and $p=0.3$ this equals 0.68.

\item {\bf Hamming}
Let $X\sim P_q^n$ and $Y\sim P_p^n$.
\begin{eqnarray*}
  d_H(P_q^n,P_q^n)&=&\min_P\E_P[D_H(X,Y)]
  \\   &=& \min_P\sum_{x,y}\P[X=x,Y=y]D_H(x,y)
  \\ &=& \min_P\sum_{x,y}\P[X=x,Y=y]\sum_{i=1}^nD_H(x_i,y_i)
  \\ &=& \min_P\sum_{i=1}^n\sum_{x,y}\P[X=x,Y=y]D_H(x_i,y_i)
  \\ &=& \min_P\sum_{i=1}^n\sum_{x_i,y_i}\P[X_i=x_i,Y_i=y_i]D_H(x_i,y_i)
  \\ &=& \min_P\sum_{i=1}^n\E_P[D_H(X_i,Y_i)].
\end{eqnarray*}
Now, for a single bit the Hamming distance and the total variation distance
coincide, so that $D_H(X_i,Y_i)=D_{TV}(X_i,Y_i)$. Therefore
\begin{eqnarray*}
  d_H(P_1,P_2)=\min_P\sum_{i=1}^n\E_P[D_{TV}(X_i,Y_i)].
\end{eqnarray*}
By the previous question, each summand can be minimized to $q-p$, and so a
lower bound is $n(q-p)$. This bound may indeed be achieved, since $P$ can
be constructed as the product of independent distributions, one for each of the
$n$ bits, where the distribution on each bit is precisely the distribution that
minimizes the expected total variation distance between $P_q^1$ and $P_p^1$.
Hence
$d_H(P_q^n,P_q^n)=n(q-p)$.

\end{itemize}

\end{document}


















