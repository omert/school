\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{amsthm} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}

\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{proposition*}{Proposition}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}


\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}
\newcommand{\fifth}{{\textstyle \frac15}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Combinatorial Statistics - Homework Set 2 Solution}

\date{\today}
\maketitle
\subsection{Markov Random Fields}
{\bf The Markov Property}

Assume the graph is connected. If not, clearly each connected component is
independent and apply the proof below to each component.
Given $A_0$, $B_0$ and $S$ be such that $S$ separates $A_0$ and $B_0$,
we ``grow'' $A_0$ by repeatedly adding to it any vertex the neighbors it
and is not in $S$ or $A_0$. We name it $A$ in its ``adult'' state. We do
the same for $B_0$, creating $B$. It is easy to
convince oneself that $S$ separates $A$ and $B$, and that the complement of the 
union of $A$, $B$ and $S$ does not neighbor $A$ or $B$. We call this complement
$R$.

Given some $x_S$, we denote by $f_C$ the function $\psi_C$ restricted
to $\sigma_S=x_S$. Note that $f_C$ is constant when $C\subseteq S$. Let the notation $C\sim A$ 
signify the same as $C\cap A\neq\emptyset$. Then one and only one of $C\sim A$, $C\sim B$,
$C\sim R$, $C\subseteq S$  holds for a given clique $C$.

Now,
\begin{eqnarray*}
  \lefteqn{\P[\sigma_S=x_S]}
\\ &=& Z^{-1}\sum_{y_{A\cup B\cup R}}\prod_Cf_C(y_{A\cup B})
\\ &=& Z^{-1}\sum_{y_{A\cup B\cup R}}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(y_A)\prod_{C\sim B}f_C(y_B)\prod_{C\sim R}f_C(y_R)
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\sum_{y_A}\prod_{C\sim A}f_C(y_A)\sum_{z_B}\prod_{C\sim B}f_C(z_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Likewise
\begin{eqnarray*}
  \lefteqn{\P[\sigma_A=x_A,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(x_A)\sum_{z_B}\prod_{C\sim B}f_C(z_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
and
\begin{eqnarray*}
  \lefteqn{\P[\sigma_B=x_B,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim B}f_C(x_B)\sum_{z_A}\prod_{C\sim A}f_C(z_A)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Finally,
\begin{eqnarray*}
  \lefteqn{\P[\sigma_A=x_A,\sigma_B=x_B,\sigma_S=x_S]}
\\ &=& Z^{-1}\prod_{C\subseteq S}f_C\prod_{C\sim A}f_C(x_A)\prod_{C\sim B}f_C(x_B)\sum_{w_R}\prod_{C\sim R}f_C(w_R)
\end{eqnarray*}
Hence
\begin{eqnarray*}
\lefteqn{\P[\sigma_{A\cup B}=x_{A\cup B}|\sigma_S=x_S]\over  
    \P[\sigma_A=x_A|\sigma_S=x_S]\P[\sigma_B=x_B|\sigma_S=x_S]}
\\  &=&   {\P[\sigma_{A\cup B}=x_{A\cup B},\sigma_S=x_S]\P[\sigma_S=x_S]\over  
    \P[\sigma_A=x_A,\sigma_S=x_S]\P[\sigma_B=x_B,\sigma_S=x_S]}
\end{eqnarray*}
We have show that $\sigma_A$ and $\sigma_B$ are independent when conditioned
on $\sigma_S$. This implies that the same holds for $A_0$ and $B_0$:
\begin{eqnarray*}
\lefteqn{\P[\sigma_{A_0\cup B_0}=x_{A_0\cup B_0}|\sigma_S=x_S]}
\\ &=& 
\sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_{A_0\cup B_0}=x_{A_0\cup B_0},\sigma_{A\setminus A_0}=x_{A\setminus A_0},\sigma_{B\setminus B_0}=x_{B\setminus B_0}|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_{A\cup B}=x_{A\cup B}|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\sum_{x_{B\setminus B_0}}\P[\sigma_A=x_A|\sigma_S=x_S]\P[\sigma_B=x_B|\sigma_S=x_S]
\\ &=& \sum_{x_{A\setminus A_0}}\P[\sigma_A=x_A|\sigma_S=x_S]\sum_{x_{B\setminus B_0}}\P[\sigma_B=x_B|\sigma_S=x_S]
\\ &=& \P[\sigma_{A_0}=x_{A_0}|\sigma_S=x_S]\P[\sigma_{B_0}=x_{B_0}|\sigma_S=x_S]
\end{eqnarray*}


{\bf Max-Cuts}

Let max-cuts of $G$ have $k$ edges crossing the cut. Then, if $\tau$ is a particular
max-cut, then
\begin{eqnarray*}
  \P[\tau]&=&Z^{-1}\exp\left(100nk-100n(|E|-k)\right)
  \\ &=& Z^{-1}\exp\left(200nk-100n|E|\right)
\end{eqnarray*}
The probability of a cut $\sigma$ that is not maximal is at most
\begin{eqnarray*}
  \P[\sigma]&=&Z^{-1}\exp\left(100n(k-1)-100n(|E|-(k-1))\right)
  \\ &=& \P[\tau]\exp\left(-200n\right)
\end{eqnarray*}
Since there are less than $2^n=\exp(n\log 2)$ cuts that are not maximal, then
\begin{eqnarray*}
  \P[\sigma\mbox{ is not a MAX-CUT of $G$}] &\leq& \P[\tau]\exp\left(-n(200-\log 2)\right)
\\ &\leq& \exp(-n(200-\log 2)) 
\\ &\leq& \exp(-199) 
\end{eqnarray*}

\subsection{The Transportation Distance and Total Variation}
{\bf Transportation Distance}

\begin{itemize}
\item To show this we need to prove the triangle inequality and that
  $d_D(P_1,P_2)=0 \leftrightarrow P_1=P_2$.
  \begin{enumerate}
  \item Given $X\sim P_X$, $Y\sim P_Y$ and $Z\sim P_Z$, let $P_{XY}$ be
    a coupling that minimizes $\E(D(X,Y))$ and $P_{YZ}$ a coupling that
    minimizes $\E(D(Y,Z))$. Define $P_{XZ}$ by:
    $$\P_{XZ}[X=x,Z=z]=\sum_y{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}.$$ 
    Now 
    \begin{eqnarray*}
      \P_{XZ}[X=x]&=&\sum_z\P_{XZ}[X=x,Z=z]
      \\ &=& \sum_{y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}
      \\ &=& \sum_{y}{\P_{XY}[X=x,Y=y]\P_{Y}[Y=y]\over \P_Y[Y=y]}
      \\ &=& \sum_{y}\P_{XY}[X=x,Y=y]
      \\ &=& \P_X[X=x]
    \end{eqnarray*}
    and likewise $\P_{XYZ}[Z=z]=\P_Z[Z=z]$. By corollary, $P_{XZ}$ is a
    distribution, and a coupling between $X$ and $Z$. Hence 
    \begin{eqnarray*}
      d_D(P_X,P_Z)&\leq& \E_{P_{XZ}}(D(X,Z)) 
      \\ &=& \sum_{x,z}\P_{XZ}[X=x,Z=z]D(X,Z)
      \\ &=& \sum_{x,y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}D(X,Z)
      \\ &\leq&\sum_{x,y,z}{\P_{XY}[X=x,Y=y]\P_{YZ}[Y=y,Z=z]\over \P_Y[Y=y]}(D(X,Y)+D(X,Z))
      \\ &=&\sum_{x,y}\P_{XY}[X=x,Y=y]D(X,Y)+\sum_{y,z}\P_{YZ}[Y=y,Z=z]D(X,Z)
      \\ &=&d_D(P_X,P_Y)+d_D(P_Y,P_Z)
    \end{eqnarray*}
    
  \item If $P_1=P_2$ then by the trivial coupling ($X$ and $Y$ are identical)
    we have $\E(D(X,Y))=0$. 

    Assume $d_D(P_1,P_2)=0$. Then there exists a coupling $P$ such that  
    $(X,Y)\sim P$,
    $X\sim P_1$, $Y\sim P_2$ and $\E_P(D(X,Y))=0$. Hence $\P[X\neq Y]=0$ and
    $\P[X=\omega]=P[Y=\omega]$ and $X$ and $Y$ are distributed identically.
  \end{enumerate}
  \item 
    For  $X\sim P_1$, $Y\sim P_2$ and $Z\sim P_3$, let $P_{13}$ and $P_{23}$ 
    be couplings that minimize $\E(D(X,Z))$ and
    $\E(D(Y,Z))$.
    
    Define $W$ to equal $X$ with probability $\alpha$ and $Y$ with probability
    $1-\alpha$, so that $W$'s set of possible outcomes is the union of those of
    $X$ and $Y$. Then $W\sim P_\alpha$ with  $P_\alpha=\alpha P_1 +(1-\alpha)P_2$. We can likewise define a 
    coupling between $W$ and $Z$: with probability $\alpha$ couple $Z$ with
    $X$ using $P_{13}$, and with probability $1-\alpha$ couple $Z$ with $Y$
    using $P_{23}$. The joint coupled probability distirbution of $W$ and $Z$
    is hence $P_{\alpha 3}=\alpha P_{13} +(1-\alpha)P_{23}$. Then
    \begin{eqnarray*}
      \P_{\alpha 3}[W=w]&=&\alpha \P_{13}[W=w] +(1-\alpha)\P_{23}[W=w]
      \\ &=& \alpha \P_1[W=w]+(1-\alpha)\P_2[W=w]
      \\ &=& \P_\alpha[W=w]
    \end{eqnarray*}
    and
    \begin{eqnarray*}
      \P_{\alpha 3}[Z=z]&=&\alpha \P_{13}[Z=z] +(1-\alpha)\P_{23}[Z=z]
      \\ &=& \alpha \P_3[Z=z]+(1-\alpha)\P_3[Z=z]
      \\ &=& \P_3[Z=z]
    \end{eqnarray*}
    and so $P_{\alpha 3}$ is indeed a coupling of $P_\alpha$ and $P_3$. Finally
    \begin{eqnarray*}
      \lefteqn{d_D(\alpha P_1+(1-\alpha)P_2,P_3)}
      \\ &=& d_D(P_\alpha,P_3)
      \\ &\leq&  \E_{P_{\alpha 3}}D(W,Z)
      \\ &=&  \sum_{w,z}\P_{\alpha 3}[W=w,Z=z]D(W,Z)
      \\ &=& \sum_{w,z}\left(\alpha \P_{13}[W=w,Z=z] +(1-\alpha)\P_{23}[W=w,Z=z]\right)D(W,Z)
      \\ &=& \alpha \sum_{x,z}\P_{13}[W=x,Z=z]D(W,Z) +(1-\alpha)\sum_{y,z}\P_{23}[W=y,Z=z]D(W,Z)
      \\ &=& \alpha d_D(P_1,P_3)+(1-\alpha)d_D(P_2,P_3)
    \end{eqnarray*}
    \item They imply that the total variation distance is a convex metric? 
\end{itemize}

{\bf Total Variation}

Again let $X\sim P_1$ and $Y\sim P_2$. Denote $D(x,y)=1_{x\neq y}$. Then:

\begin{eqnarray*}
  d_{TV}&=&\min_{P, P_{|1}=P_1, P_{|2}=P_2}\E_P[D(X,Y)]
  \\ &=& \min_{P, P_{|1}=P_1, P_{|2}=P_2}\sum_{x,y}\P[X=x,Y=y]D(x,y)
  \\ &=& \min_{P, P_{|1}=P_1, P_{|2}=P_2}\sum_{x,y}\P[X=x,Y=y]1_{x\neq y}
  \\ &=& \min_{P, P_{|1}=P_1, P_{|2}=P_2}\P[x\neq y]
\end{eqnarray*}

{\bf An Example}

The best coupling possible between $X\sim P_q^n$ and $Y\sim P_p^n$ can be 
thought of as picking $n$ reals $\{r_i\}$ from the uniform distribution on 
$[0,1]$, and then setting $X_i=1$ iff $r_i>q$ and $Y_i=1$ iff $r_i>p$ 
(shown in class for $n=1$ and the Hamming distance, but also true 
for both Hamming and total variation distances with $n\geq 1$). 
The probability that $X_i= Y_i$ for some $i$ is $q-p$. 
\begin{itemize}
\item We use here the last result on total variation distance:
\begin{eqnarray*}
  d_{TV}(P_q^n,P_p^n) = \P[X\neq Y]=1-\prod_{i=1}^n\P[X_i= Y_i]=1-(q-p)^n.
\end{eqnarray*}
\item
This calculation can be easily done in the case of Hamming distance, be virtue
of the additivity of expectation, matched with the additivity of the Hamming 
distance: $D_H(x,y)=\sum_iD_H(x_i,y_i)$:
\begin{eqnarray*}
  d_H(P_q^n,P_p^n) &=& \E_p[\mbox{\# of bits by which $x$ and $y$ differ}]
  \\ &=& \sum_i\E_p[\mbox{\# of bits by which $x_i$ and $y_i$ differ}]
  \\ &=& \sum_i1-(q-p)
  \\ &=& n(1-(q-p))
\end{eqnarray*}

\end{itemize}
\end{document}


















