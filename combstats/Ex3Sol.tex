\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{amsthm} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}

\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{proposition*}{Proposition}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}


\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}
\newcommand{\fifth}{{\textstyle \frac15}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Combinatorial Statistics - Homework Set 3 Solution}

\date{\today}
\maketitle
\subsection{Sampling Uniform Colorings}
\begin{itemize}
\item Arbitrarily order the vertices $v_1,\ldots,v_n$. Iterating over $i$,
assign to $v_i$ a color not assigned to any of its neighbors $v_j$ such that
$j<i$. Since the degree of $v_i$ is at most $d$, it has at most $d$ such
neighbors, and so one of the $q\geq d+1$ colors will be available.

\item Let $\sigma_0$ and $\tau_0$ be legal colorings that differ in exactly
one vertex. 
Let $X_0,Y_0$ be the distributions over the legal colorings for which  
$\P_{X_0}[\sigma_0]=1$ and $\P_{Y_0}[\tau_0]=1$. Let  $X_1,Y_1$ be the distributions resulting from the
application of the Gibbs sampler to $X_0$ and $Y_0$, respectively. The Gibbs
sampler here, as expected, picks a vertex $v$ uniformly at random and then 
picks, again uniformly at random, a legal color for $v$.

Define a metric $H$ on legal colorings by  
$H(\sigma_0,\tau_0)=|\{v\in V :\:\sigma_0(v)\neq \tau_0(v)\}|$. This is a generalization of the Hamming 
distance to multi-valued vectors. We would like to show the following:
\begin{proposition*}
  $d_H(X_1,X_2)\leq 1-{1\over n(d+1)}$.
\end{proposition*}
Note that by a proof identical to the one shown in class for the binary Hamming
distance, this would imply that indeed this sampler runs in
$O(dn(\log n - \log \epsilon))$.
 
\begin{proof}
  We define a coupling $Q$ between $X_1$ and $Y_1$, 
  by applying the Gibbs sampler to $\sigma_0$ and $\tau_0$, arriving at 
  $\sigma_1$ and $\tau_1$, as follows:
  \begin{enumerate}
  \item Pick a vertex $v$ uniformly at random. For $u\neq v$, let 
    $\sigma_1(u)=\sigma_0(u)$ and $\tau_1(u)=\tau_0(u)$.
  \item Let $w$ be the vertex for which $\sigma_0$ and $\tau_0$ differ. Let 
    $C_\sigma$ be the legal colors for $v$ under $\sigma_0$, and define
    $C_\tau$ similarly.
    \begin{enumerate}
    \item If $v=w$ or $v \not \in N(w)$ then $C_\sigma=C_\tau=C$. Pick for $v$ a legal 
      color $c\in C$ uniformly at random and set $\sigma_1(v)=\tau_1(v)=c$.
    \item If $v \in N(w)$ then assume WLOG that $|C_\sigma|\geq |C_\tau|$.
      Then either $C_\tau=C_\sigma$,  $C_\tau=C_\sigma\setminus \{\tau_0(w)\}$ or
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}\cup \{\sigma_0(w)\}$.
      Pick uniformly at random
      a color $c\in C_\sigma$, and set $\sigma_1(v)=c$. If $c\in C_\tau$ then
      set $\tau_1(v)=c$ too. Otherwise $c=\tau_0(w)$; in this case, if  
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}$ 
      then pick $\tau_1(v)$ uniformly at random from $C_\tau$, and if 
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}\cup \{\sigma_0(w)\}$ then
      set $\tau_1(v)=\sigma_0(w)$. Note that $|C_\sigma|\geq q-d \geq 2d+1-d =d+1$, and
      so the probability that $c=\tau_0(w)$ and hence 
      $\tau_1(w)\neq  \sigma_1(w)$ is at most
      $1/(d+1)$.
    \end{enumerate}
  \end{enumerate}
  It's easy to convince oneself that the resulting marginal distributions 
  of $\sigma_1$ and $\tau_1$ are indeed $X_1$ and $Y_1$, respectively. Furthermore:
  \begin{align*}
    \E_Q[H(X_1,Y_1)] &= \P_Q[v=w]\cdot 0+\P_Q[v\not \in \{w\}\cup N(w)]\cdot 1
    \\ & \quad\quad + \P_Q[v\in N(w)](\P_Q[\sigma_1(v)=\tau_1(v)]\cdot 1 + \P_Q[\sigma_1(v)\neq\tau_1(v)]\cdot 2)
    \\  &= 1-{1\over n}-{d(w)\over n}+{d(w)\over n}((1-\P_Q[\sigma_1(v)\neq\tau_1(v)]) +2\P_Q[\sigma_1(v)\neq\tau_1(v)])
    \\  &= 1-{1\over n}-{d(w)\over n}+{d(w)\over n}(1+\P_Q[\sigma_1(v)\neq\tau_1(v)])
    \\  &\leq 1-{1\over n}-{d(w)\over n}+{d(w)\over n}\left(1+{1\over d(w)+1}\right)
    \\  &= 1-{1\over n}\cdot{1\over d(w)+1}
    \\  &\leq 1-{1\over n(d+1)}
  \end{align*}
\end{proof}


\item
For $n \geq 2$, let $G_n$ be he clique on $n$ vertices. Then the maximal degree
of $G_n$ is $n-1$, and $q_n=n$. Since any two of the $n!$ legal colorings on 
$G_n$ have
Hamming distance $n>1$, then a Gibbs sampler started on a particular coloring
will never leave it, and in particular does not provide $1/4$ sampling after
any amount of time.


\end{itemize}
\subsection{Other Statistical Quantities}
After running the Markov chain for $t(\epsilon)$ iterations we get a state
$x \sim X$ such that $d_{TV}(X,Q)<\epsilon$. 

Now
\begin{align*}
  \E_X[f-1/2]-E_Q[f-1/2] &=\E_X[f]-\E_X[1/2]-E_Q[f]+E_Q[1/2]
  \\ &= \E_X[f]-1/2-E_Q[f]+1/2
  \\ &= \E_X[f]-E_Q[f].
\end{align*}
Therefore:
\begin{align*}
  |\E_X[f]-E_Q[f]| &= |\E_X[f-1/2]-E_Q[f-1/2]|
  \\ &= \left|\sum_x\P_X[x](f(x)-1/2)-\sum_x\P_Q[x](f(x)-1/2)\right|
  \\ &= \left|\sum_x(\P_X[x]-\P_Q[x])(f(x)-1/2)\right|
  \\ &\leq \sum_x\left|\P_X[x]-\P_Q[x]\right| |f(x)-1/2|
  \\ &\leq {1\over 2}\sum_x\left|\P_X[x]-\P_Q[x]\right|
  \\ &\leq \epsilon
\end{align*}
Now, for $N$ iterations run the Markov chain $t(\epsilon)$ times from a random
starting point. Denote the final states of the iterations by
$x_1,\ldots,x_N$. Then the $x_i$ are independent and distributed according
to $X$, and hence also $f(x_i)$ are independent, with expectation $\E_X[f]$. Our
algorithm $A$ returns the average of $f(x_i)$, $(1/N)\sum_i f(x_i)$.

Since $0 \leq f(x_i) \leq 1$, we can use the Hoeffding inequality:
\begin{align*}
  \P\left[\left|\sum_if(x_i)-N\E_X[f]\right|\geq N\epsilon\right] &\leq 
    2\exp\left(-{2N^2\epsilon^2\over N}\right)
\\   \P\left[\left|A-\E_X[f]\right|\geq \epsilon\right] &\leq 
    2\exp\left(-2N\epsilon^2\right)
\end{align*}
Since $|\E_X[f]-E_Q[f]|<\epsilon|$, then this means that
\begin{align*}
   \P\left[\left|A-\E_Q[f]\right|\geq 2\epsilon\right] &\leq 
    2\exp\left(-2N\epsilon^2\right)
\end{align*}
and so to achieve $\P\left[\left|A-\E_Q[f]\right| < 2\epsilon\right]>1-\delta$
we must have
\begin{align*}
  2\exp(-2N\epsilon^2)&<\delta
  \\ -2N\epsilon^2&<\log\delta-\log 2
  \\ N &> {\log 1/\delta+\log 2\over \epsilon^2}.
\end{align*}
\subsection{Bottlenecks and Mixing Times}
\begin{itemize}
\item 
  We follow here a similar proof given in class.

  Pick a state $\sigma_0$ from the stationary distribution $Q$.
  Let $X_0$ be the distribution for which 
  $\P_{X_0}[\sigma_0]=1$. Let $T$ be the Gibbs sampler's transition matrix, 
  and apply the Gibbs sampler to $X_0$. Then:
  \begin{align*}
    \P[X_1=\tau] &= \P_{\sigma_0}[X_1=\tau|\sigma_0]
    \\ &= \E_{\sigma_0}[\P[X_1=\tau|X_0=\sigma_0]]
    \\ &= \E_{\sigma_0}[T(\sigma_0\to\tau)]
    \\ &= \sum_{\sigma_0}Q(\sigma_0)T(\sigma_0\to\tau)
    \\ &= \sum_{\sigma_0}T(\sigma_0\to\tau)Q(\sigma_0)
    \\ &= Q(\tau)
  \end{align*}
  where the last equality holds because $Q$ is a stationary distribution of $T$.
  We can use this argument inductively to show 
  that the marginal distributions of all the
  $X_t$'s are $Q$. 

  This implies that the probability of $X_t\in S$ is $Q(S)=s$,
  and by union-bound the chance that one of $X_0,\ldots,X_{t-1}$ is in $S$ is
  at most $st$: $\P[\exists t'<t:X_{t'}\in S]\leq st$. We can rewrite this as 
  $\E_{\sigma_0}[\P_X[\exists t'<t:X_{t'}\in S|\sigma_0]] \leq st$, which 
  implies that there exists a $\sigma_0$ for which 
  $\P_X[\exists t'<t:X_{t'}\in S]<st$. Now, this $\sigma_0$ is not in $S$, since in that
  case the probability is one. Assume then WLOG that $\sigma_0\in A$. Then,
  since visiting $S$ at some point is a prerequisite for visiting $B$, we have
  that the probability that $X_t$ is in $B$ is also less than $st$, and so:
  \begin{align*}
    d_{TV}(X_t,Q)\geq Q(B)-X_t(B) \geq m-st.
  \end{align*}
\item

  Suppose $d_{TV}(X_t,Y_t)\leq u < 1$ for all starting points $X_0$, $Y_0$. Denote
  by $X_t^\omega$ the distribution achieved $t$ steps after starting at 
  $\omega$. Then between every $X_t^\omega$ and $Y_t^\mu$ there exists a coupling 
  $P_{\omega\mu}$ such that $\P_{\omega\mu}[x\neq y] < u$.

  We prove the proposition by induction. The base ($r=1$) is given. Assume then
  that there exists a coupling $P_r$ 
  between $X_{rt}$ and $Y_{rt}$ such that $\P_r[x\neq y] < u^r$.
  Define a coupling $P_{r+1}$ for  $X_{(r+1)t}$ and $Y_{(r+1)t}$ as follows: for the
  first $rt$ steps, couple by $P_r$. Denote the states arrived at by $X_{rt}$ and
  $Y_{rt}$ as $\omega$ and $\mu$, respectively. If $\omega=\mu$, then
  continue the process for $t$ more steps by making identical updates to $X$
  and $Y$. Otherwise, 
  couple by $P_{\omega\mu}$ for the next $t$ steps.

  With probability at least $1-u^r$ we'll have $\omega=\mu$, and then also 
  after $(r+1)t$ steps the states will be identical.
  With probability at most $u^r$ we'll have $\omega\neq\mu$. In this case the final states
  will be different with conditional probability at most $u$. Therefore
  $\P_{r+1}[x\neq y]\leq u^{r+1}$, and the proposition follows.

\item Assume by way of contradiction that for all $X_0$, $Y_0$ there exists a
  $t\leq m/(4\log_2(1/m)s)$ such that $d_{TV}(X_t,Y_t)<1/2^{1/2}$. In the
  first proof above we show that 
  \begin{align*}
    d_{TV}(X_{rt},Y_{rt}) &\geq  m-srt
    \\ &\geq m-srm/(4\log_2(1/m)s)
    \\ &= m-rm/(4\log_2(1/m)).
  \end{align*}
  In the second proof we show that
  \begin{align*}
    d_{TV}(X_{rt},Y_{rt}) &\leq  1/2^{r/2}.
  \end{align*}
  Both of these hold when $1/2^{r/2} \leq m-rm/(4\log_2(1/m))$. However, if we set
  $r=4\log_2(1/m)$ then this becomes:
  \begin{align*}
    1/2^{r/2} &\leq m-rm/(4\log_2(1/m))
    \\ 1/2^{2\log_2(1/m)} &\leq 0
  \end{align*}
  which is clearly a contradiction.

  We have shown that for some $X_0$, $Y_0$ and $t = m/(4\log_2(1/m)s)$ we
  have that $d_{TV}(X_t,Y_t) \geq 1/2^{1/2}$. Then:
  \begin{align*}
    d_{TV}(X_t,Y_t) \leq d_{TV}(X_t,Q) + d_{TV}(Q,Y_t)
  \end{align*}
  and so WLOG $d_{TV}(X_t,Q) \geq 1/2\cdot 1/2^{1/2}>1/4$. Now $d_{TV}(X_t,Q)$ is monotonically
  decreasing in $t$ (couple $X_t$ and $Q$ using the optimal coupling for $X_{t-1}$
  and $Q$, as in the second proof), and therefore
  this holds for all  $t \leq m/(4\log_2(1/m)s)$.
\end{itemize}


\end{document}


















