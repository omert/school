\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{amsthm} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}

\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{proposition*}{Proposition}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}


\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}
\newcommand{\fifth}{{\textstyle \frac15}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Combinatorial Statistics - Homework Set 3 Solution}

\date{\today}
\maketitle
\subsection{Sampling Uniform Colorings}
\begin{itemize}
\item Arbitrarily order the vertices $v_1,\ldots,v_n$. Iterating over $i$,
assign to $v_i$ a color not assigned to any of its neighbors $v_j$ such that
$j<i$. Since the degree of $v_i$ is at most $d$, it has at most $d$ such
neighbors, and so one of the $q\geq d+1$ colors will be available.

\item Let $\sigma_0$ and $\tau_0$ be legal colorings that differ in exactly
one vertex. 
Let $X_0,Y_0$ be the distributions over the legal colorings for which  
$\P_{X_0}[\sigma_0]=1$ and $\P_{Y_0}[\tau_0]=1$. Let  $X_1,Y_1$ be the distributions resulting from the
application of the Gibbs sampler to $X_0$ and $Y_0$, respectively. The Gibbs
sampler here, as expected, picks a vertex $v$ uniformly at random and then 
picks, again uniformly at random, a legal color for $v$.

Define a metric $H$ on legal colorings by  
$H(\sigma_0,\tau_0)=|\{v\in V :\:\sigma_0(v)\neq \tau_0(v)\}|$. This is a generalization of the Hamming 
distance to multi-valued vectors. We would like to show the following:
\begin{proposition*}
  $d_H(X_1,X_2)\leq 1-{1\over n(d+1)}$.
\end{proposition*}
Note that by a proof identical to the one shown in class for the binary Hamming
distance, this would imply that indeed this sampler runs in
$O(dn(\log n - \log \epsilon))$.
 
\begin{proof}
  Let $Q$ be the coupling between $X_1$ and $Y_1$ used for the Ising model, 
  realized by applying the Gibbs sampler to $\sigma_0$ and $\tau_0$, arriving at 
  $\sigma_1$ and $\tau_1$, as follows:
  \begin{enumerate}
  \item Pick a vertex $v$ uniformly at random. For $u\neq v$, let 
    $\sigma_1(u)=\sigma_0(u)=\tau_0(u)=\tau_1(u)$.
  \item Let $w$ be the vertex in which $\sigma_0$ and $\tau_0$ differ. Let 
    $C_\sigma$ be the legal colors for $v$ under $\sigma_0$, and define
    $C_\tau$ similarly.
    \begin{enumerate}
    \item If $v=w$ or $v \not \in N(w)$ then $C_\sigma=C_\tau=C$. Pick for $v$ a legal 
      color $c$ from $C$ uniformly at random and set $\sigma_1(v)=\tau_1(v)=c$.
    \item If $v \in N(w)$ then assume WLOG that $|C_\sigma|\geq |C_\tau|$.
      Then either $C_\tau=C_\sigma$,  $C_\tau=C_\sigma\setminus \{\tau_0(w)\}$ or
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}\cup \{\sigma_0(w)\}$.
      Pick uniformly at random
      a color $c\in C_\sigma$, and set $\sigma_1(v)=c$. If $c\in C_\tau$ then
      set $\tau_1(v)=c$ too. Otherwise $c=\tau_0(w)$; in this case, if  
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}$ 
      then pick $\tau_1(v)$ uniformly at random from $C_\tau$, and if 
      $C_\tau=C_\sigma\setminus \{\tau_0(w)\}\cup \{\sigma_0(w)\}$ then
      set $\tau_1(v)=\sigma_0(w)$. Note that $|C_\sigma|\geq q-d \geq 2d+1-d =d+1$, and
      so the probability that $c=\tau_0(w)$ and hence 
      $\tau_1(w)\neq  \sigma_1(w)$ is at most
      $1/(d+1)$.
    \end{enumerate}
  \end{enumerate}
  It's easy to convince oneself that the resulting marginal distributions 
  of $\sigma_1$ and $\tau_1$ are indeed $X_1$ and $Y_1$, respectively. Furthermore:
  \begin{align*}
    \E_Q[H(X_1,Y_1)] &= \P_Q[v=w]\cdot 0+\P_Q[v\not \in \{w\}\cup N(w)]\cdot 1
    \\ & \quad\quad + \P_Q[v\in N(w)](\P_Q[\sigma_1(v)=\tau_1(v)]\cdot 1 + \P_Q[\sigma_1(v)\neq\tau_1(v)]\cdot 2)
    \\  &= 1-{1\over n}-{d(w)\over n}+{d(w)\over n}((1-\P_Q[\sigma_1(v)\neq\tau_1(v)]) +2\P_Q[\sigma_1(v)\neq\tau_1(v)])
    \\  &= 1-{1\over n}-{d(w)\over n}+{d(w)\over n}(1+\P_Q[\sigma_1(v)\neq\tau_1(v)])
    \\  &\leq 1-{1\over n}-{d(w)\over n}+{d(w)\over n}\left(1+{1\over d(w)+1}\right)
    \\  &= 1-{1\over n}\cdot{1\over d(w)+1}
    \\  &\leq 1-{1\over n(d+1)}
  \end{align*}
\end{proof}


\item
For $n \geq 2$, let $G_n$ be he clique on $n$ vertices. Then the maximal degree
of $G_n$ is $n-1$, and $q_n=n$. Since any two of the $n!$ legal colorings on 
$G_n$ have
Hamming distance $n>1$, then a Gibbs sampler started on a particular coloring
will never leave it, and in particular does not provide $1/4$ sampling after
any amount of time.


\end{itemize}
\subsection{Other Statistical Quantities}
After running the Markov chain for $t(\epsilon)$ iterations we get a state
$x \sim X$ such that $d_{TV}(X,Q)<\epsilon$. 

Now
\begin{align*}
  \E_X[f-1/2]-E_Q[f-1/2] &=\E_X[f]-\E_X[1/2]-E_Q[f]+E_Q[1/2]
  \\ &= \E_X[f]-1/2-E_Q[f]+1/2
  \\ &= \E_X[f]-E_Q[f].
\end{align*}
Therefore:
\begin{align*}
  |\E_X[f]-E_Q[f]| &= |\E_X[f-1/2]-E_Q[f-1/2]|
  \\ &= \left|\sum_x\P_X[x](f(x)-1/2)-\sum_x\P_Q[x](f(x)-1/2)\right|
  \\ &= \left|\sum_x(\P_X[x]-\P_Q[x])(f(x)-1/2)\right|
  \\ &\leq \sum_x\left|\P_X[x]-\P_Q[x]\right| |f(x)-1/2|
  \\ &\leq {1\over 2}\sum_x\left|\P_X[x]-\P_Q[x]\right|
  \\ &\leq \epsilon
\end{align*}
Now, for $N$ iterations run the Markov chain $t(\epsilon)$ times and take
the last state $x$ reached every time. Denote those states reached
$x_1,\ldots,x_N$. Then the $x_i$ are independent and distributed according
to $X$, and hence also $f(x_i)$ are independent, with expectation $\E_X[f]$. Our
algorithm $A$ returns the average of $f(x_i)$, $(1/N)\sum_i f(x_i)$.

Since $0 \leq f(x_i) \leq 1$, we can use the Hoeffding inequality:
\begin{align*}
  \P\left[\left|\sum_if(x_i)-N\E_X[f]\right|\geq N\epsilon\right] &\leq 
    2\exp\left(-{2N^2\epsilon^2\over N}\right)
\\   \P\left[\left|A-\E_X[f]\right|\geq \epsilon\right] &\leq 
    2\exp\left(-2N\epsilon^2\right)
\end{align*}
Since $|\E_X[f]-E_Q[f]|<\epsilon|$, then this means that
\begin{align*}
   \P\left[\left|A-\E_Q[f]\right|\geq 2\epsilon\right] &\leq 
    2\exp\left(-2N\epsilon^2\right)
\end{align*}
and so to achieve $\P\left[\left|A-\E_Q[f]\right| < 2\epsilon\right]>1-\delta$
we must have
\begin{align*}
  2\exp(-2N\epsilon^2)&<\delta
  \\ -2N\epsilon^2&<\log\delta-\log 2
  \\ N &> {\log 1/\delta+\log 2\over \epsilon^2}.
\end{align*}
\subsection{Bottlenecks and Mixing Times}
\begin{itemize}
\item 
  Assume WLOG that $Q(A)=m$. 
  Let $X_0=Q_{S\cup A}$ be the distribution $Q$ conditioned on $\omega\in S\cup A$. Then, by 
  $d_{TV}(X,Y)=\sup_A\{|X(A)-Y(A)|\}$, we have that 
  $d_{TV}(X_0,Q)\geq Q(B)-X_0(B)\geq m-0=m$. 
  
  Let $p$ be the probability that
  the Gibbs sampler will transition from $S\cup A$ to $B$ when its distribution
  is $X_0$. Then $p\leq X_0(S) = s/(s+m)$, since this transition is 
  possible only from $S$. Now, as long as the sampler stays in $S\cup A$,
  the distribution of its state remains $X_0$, since it is a stationary
  distribution on these sets. The probability that this indeed happens in the
  first $t$ steps is hence $(1-p)^t$, which is at least $1-pt$, which in turn
  is at least $1-st/(s+m)$.
  Therefore the probability that it ever reaches $B$ at time $t$ is at most
  $st/(s+m)$, and so $X_t(B)\leq st/(s+m)$. 
  Hence $d_{TV}(X_t,Q)\geq Q(B)-X_t(B)\geq Q(B)-st/(s+m) \geq m-st/(s+m)$.

\item
  Suppose $d_{TV}(X_t,Y_t)\leq u < 1$ for all starting points $X_0$, $Y_0$. Denote
  by $X_t^\omega$ the distribution achieved $t$ steps after starting at 
  $\omega$.

  Assume the conjecture is true for $r-1$. Then:
  \begin{align*}
    d_{TV}(X_{rt},Y_{rt}) &= {1\over 2}\sum_\omega |X_{rt}(\omega)-Y_{rt}(\omega)|
    \\ &= {1\over 2}\sum_\omega \left|\sum_{\omega'}X_{(r-1)t}(\omega')X_t^{\omega'}(\omega)-\sum_{\omega'}Y_{(r-1)t}(\omega')Y_t^{\omega'}(\omega)\right|
    \\ &\leq {1\over 2}\sum_\omega \sum_{\omega'}\left|X_{(r-1)t}(\omega')X_t^{\omega'}(\omega)-Y_{(r-1)t}(\omega')Y_t^{\omega'}(\omega)\right|
    \\ &\leq {1\over 2}\sum_{\omega'} \left(\sum_\omega\left|X_{(r-1)t}(\omega')X_t^{\omega'}(\omega)-Y_{(r-1)t}(\omega')Y_t^{\omega'}(\omega)\right|\right)
  \end{align*}
\end{itemize}


\end{document}


















