\documentclass[11pt]{article} \usepackage{amssymb}
\usepackage{amsfonts} \usepackage{amsmath} \usepackage{amsthm} \usepackage{bm}
\usepackage{latexsym} \usepackage{epsfig}

\setlength{\textwidth}{6.5 in} \setlength{\textheight}{8.25in}
\setlength{\oddsidemargin}{0in} \setlength{\topmargin}{0in}
\addtolength{\textheight}{.8in} \addtolength{\voffset}{-.5in}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{proposition*}{Proposition}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
%\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}

\newcommand{\ignore}[1]{}

\newcommand{\enote}[1]{} \newcommand{\knote}[1]{}
\newcommand{\rnote}[1]{}


\DeclareMathOperator{\Support}{Supp} \DeclareMathOperator{\Opt}{Opt}
\DeclareMathOperator{\Ordo}{\mathcal{O}}
\newcommand{\MaxkCSP}{\textsc{Max $k$-CSP}}
\newcommand{\MaxkCSPq}{\textsc{Max $k$-CSP$_{q}$}}
\newcommand{\MaxCSP}[1]{\textsc{Max CSP}(#1)} \renewcommand{\Pr}{{\bf
    P}} \renewcommand{\P}{{\bf P}} \newcommand{\Px}{\mathop{\bf P\/}}
\newcommand{\E}{{\bf E}} \newcommand{\Cov}{{\bf Cov}}
\newcommand{\Var}{{\bf Var}} \newcommand{\Varx}{\mathop{\bf Var\/}}

\newcommand{\bits}{\{-1,1\}}

\newcommand{\nsmaja}{\textstyle{\frac{2}{\pi}} \arcsin \rho}

\newcommand{\Inf}{\mathrm{Inf}} \newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}

\newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda}

% \newcommand{\trunc}{\ell_{2,[-1,1]}}
\newcommand{\trunc}{\zeta} \newcommand{\truncprod}{\chi}

\newcommand{\N}{\mathbb N} \newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z} \newcommand{\CalE}{{\mathcal{E}}}
\newcommand{\CalC}{{\mathcal{C}}} \newcommand{\CalM}{{\mathcal{M}}}
\newcommand{\CalR}{{\mathcal{R}}} \newcommand{\CalS}{{\mathcal{S}}}
\newcommand{\CalV}{{\mathcal{V}}}
\newcommand{\CalX}{{\boldsymbol{\mathcal{X}}}}
\newcommand{\CalG}{{\boldsymbol{\mathcal{G}}}}
\newcommand{\CalH}{{\boldsymbol{\mathcal{H}}}}
\newcommand{\CalY}{{\boldsymbol{\mathcal{Y}}}}
\newcommand{\CalZ}{{\boldsymbol{\mathcal{Z}}}}
\newcommand{\CalW}{{\boldsymbol{\mathcal{W}}}}
\newcommand{\CalF}{{\mathcal{Z}}}
% \newcommand{\boldG}{{\boldsymbol G}}
% \newcommand{\boldQ}{{\boldsymbol Q}}
% \newcommand{\boldP}{{\boldsymbol P}}
% \newcommand{\boldR}{{\boldsymbol R}}
% \newcommand{\boldS}{{\boldsymbol S}}
% \newcommand{\boldX}{{\boldsymbol X}}
% \newcommand{\boldB}{{\boldsymbol B}}
% \newcommand{\boldY}{{\boldsymbol Y}}
% \newcommand{\boldZ}{{\boldsymbol Z}}
% \newcommand{\boldV}{{\boldsymbol V}}
\newcommand{\boldi}{{\boldsymbol i}} \newcommand{\boldj}{{\boldsymbol
    j}} \newcommand{\boldk}{{\boldsymbol k}}
\newcommand{\boldr}{{\boldsymbol r}}
\newcommand{\boldsigma}{{\boldsymbol \sigma}}
\newcommand{\boldupsilon}{{\boldsymbol \upsilon}}
\newcommand{\hone}{{\boldsymbol{H1}}}
\newcommand{\htwo}{\boldsymbol{H2}}
\newcommand{\hthree}{\boldsymbol{H3}}
\newcommand{\hfour}{\boldsymbol{H4}}


\newcommand{\sgn}{\mathrm{sgn}} \newcommand{\Maj}{\mathrm{Maj}}
\newcommand{\Acyc}{\mathrm{Acyc}}
\newcommand{\UniqMax}{\mathrm{UniqMax}}
\newcommand{\Thr}{\mathrm{Thr}} \newcommand{\littlesum}{{\textstyle
    \sum}}

\newcommand{\half}{{\textstyle \frac12}}
\newcommand{\third}{{\textstyle \frac13}}
\newcommand{\fourth}{{\textstyle \frac14}}
\newcommand{\fifth}{{\textstyle \frac15}}

\newcommand{\Stab}{\mathbb{S}}
\newcommand{\StabThr}[2]{\Gamma_{#1}(#2)}
\newcommand{\StabThrmin}[2]{{\underline{\Gamma}}_{#1}(#2)}
\newcommand{\StabThrmax}[2]{{\overline{\Gamma}}_{#1}(#2)}
\newcommand{\TestFcn}{\Psi}

\renewcommand{\phi}{\varphi}

\begin{document}
\title{Combinatorial Statistics - Homework Set 5 Solution}

\date{\today}
\maketitle
\subsection{Clustering}
\begin{itemize}
\item The expectation of the degree $d_v$ of any vertex $v$ in $V_i$ is:
  \begin{equation*}
    \E[d_v]=\sum_{w \in V}\P[(v,w)\in E]=\sum_jP_{i,j}|V_j|.
  \end{equation*}
  This depends only on $i$ and so we denote $d_i^\ast:=\sum_jP_{i,j}|V_j|$.
  
  By Hoeffding's inequality, for every $t$ and every vertex $v$ in
  $V_i$ with degree $d_v$ it holds that
  \begin{equation*}
    \P[|d_v-d_i^\ast|>t] \leq 2\exp(-2t^2/n).
  \end{equation*}
  If we set $t=n\eps/8$ we get for $v,u$ in the same cluster that
  \begin{equation*}
    \P[|d_v-d_u|>n\eps/4] \leq 4\exp(-n\eps^2/32).
  \end{equation*}
  
  It is given that for every $i \neq j$ it holds that
  $|d_i^\ast-d_j^\ast|>n\eps$. Therefore for $u,v$ in different
  clusters we know that
  \begin{equation*}
    \P[|d_v-d_u|<3n\eps/4] \leq 4\exp(-n\eps^2/32).
  \end{equation*}
  and so with probability at most  $4n^2\exp(-n\eps^2/32)$ this will
  hold for all vertices and so we can cluster in linear time by
  putting $u$ and $v$ in the same set whenever the difference
  between their degrees is at most $n\eps/4$.

\item
  Let $c_{v,u}$ be the number of common neighbors of $v$ and
  $u$. Assuming $v \in V_i$ and $u \in V_j$, then
  \begin{align*}
    \E[c_{v,u}]&=\sum_{w \in V}\P[(v,w) \in E\mbox{ and } (u,w) \in E]
    \\ &=\sum_{w \in V}\P[(v,w) \in E]\P[(u,w) \in E]
    \\ &=\sum_m|V_m|P_{i,m}P_{j,m}
  \end{align*}
  We can thus define $c_{i,j}^\ast:=\sum_m|V_m|P_{i,m}P_{j,m}$, and further,
  when $i=j$ then $c_i^\ast:=\sum_m|V_m|P_{i,m}^2$. Denote
  $C^\ast=\max\{c_i^\ast|i \in [k]\}$.

  Let $j \neq i$. Then
  \begin{align*}
    c_i^\ast + c_j^\ast - 2c_{i,j}^\ast &= \sum_m|V_m|P_{i,m}^2+\sum_m|V_m|P_{j,m}^2-2\sum_m|V_m|P_{i,m}P_{j,m}
    \\ &= \sum_m|V_m|(P_{i,m}-P_{j,m})^2
    \\ &= n\sum_m{|V_m| \over n}(P_{i,m}-P_{j,m})^2
    \\ &\geq n \alpha \sum_m(P_{i,m}-P_{j,m})^2
    \\ &\geq n \alpha \eps
  \end{align*}
  where the last step follows from the problem's hypothesis:
  $\sum_m(P_{i,m}-P_{j,m})^2<\eps$. Note that this means in particular
  that $c^\ast-c_{i,j}^\ast \geq \half n \alpha \eps$.

  To cluster, we calculate $c_{v,u}$ for $n/\alpha$ pairs of vertices
  picked at random. This takes $O(n^2)$ time.  Let $i$ be such that
  $c_i^\ast=c^\ast$. Then the expected number of pairs $u,v$ that are
  both in $V_i$ is at least $n$, and so the chance that no pairs have
  both vertices in $V_i$ is exponentially small in $n$.

  Let $u,v$ be such that $u \in V_j$ and $v \in V_m$, with $j \neq
  m$. Then
  \begin{align*}
    \P[c_{u,v}>c_{j,m}^\ast+ n \alpha \eps /4] < \exp(-n\alpha^2\eps^2/8)
  \end{align*}
  however, by the above there must exist two vertices $u',v'\in V_i$,
  and for these 
  \begin{align*}
    \P[c_{u',v'}<c^\ast- n \alpha \eps /4] < \exp(-n\alpha^2\eps^2/8).
  \end{align*}
  Since we showed above that $c^\ast>c_{j,m}^\ast+\half n \alpha
  \eps$, then the vertices $u,v$ with maximal $c_{u,v}$ are with
  probability at least $1-2\exp(-n\alpha^2\eps^2/8)$ in the same set
  of the partition. Denote the set they are in $V_i$. Then with
  probability $1-\exp(-n\alpha^2\eps^2/8)$ we have that $c_i>c^\ast- n
  \alpha \eps /4$.

  Having found these $u,v$ that belong to the said $V_i$, we
  calculate, for each vertex $w\in V$, the number of the common
  neighbors it has with $u$ (this also takes time $O(n^2)$). This
  number not be below $<c_i^\ast- n \alpha \eps /2$ with probability
  $1-\exp(-n\alpha^2\eps^2/8)$ whenever $w$ is also in $V_i$. Hence
  with probability $1-n\exp(-n\alpha^2\eps^2/8)$ we will find all the
  vertices of $V_i$. We can now remove them from the graph and repeat
  $k$ times to find $V_1,\ldots,V_k$. Each of these $k$ iterations
  also takes $O(n^2)$, and so the running time is linear in the size
  of the graph.
\end{itemize}


\subsection{Message Passing Algorithms for Erasure Codes}
\begin{itemize}
\item The algorithm is correct since each update that it makes is
  sure, by definition, to set the correct value for the updated
  variable. Hence if it terminates it sets the correct value for all
  variables for which this value was initially erased.
\item If we identify the variables in this problem to the vertices in
  BP, and the equations in this problem to the cliques in BP, then
  this algorithm is similar to BP: the cliques send messages to the
  variables (either giving them no information or telling them exactly
  what to be), and the variables send messages to the cliques (again
  either giving no information or telling them the exact value).

  The algorithm is not equivalent to BP in the sense that... {\bf
    Elchanan: I'm not sure why...}
\item If the algorithm has the said property, then when $|S|<\delta n$
  it will, in every iteration, have at least one variable to update,
  up until all variables are updated and the algorithm has recovered $x$.
\item The code $\{00,11\}$ is generated by the linear equation
  $x_1 \oplus x_2=0$. This code can be (and is) recovered whenever at
  least one bit is not erased.
\item Consider the code $\{000\}$ generated by $x_1 \oplus x_2$,
  $x_1\oplus x_3$ and $x_1\oplus x_2 \oplus x_3$. It is easy to verify
  that these equation do generate the single codeword $\{000\}$. Thus,
  it is possible to recover this codeword even when all bits are
  erased, which the algorithm clearly won't do, as more than one bit
  will be missing for all equations.
\end{itemize}

\subsection{Metrics on Trees}
\begin{itemize}
\item
  \begin{align*}
    \E[\sigma_u\sigma_v] &=
    {(1)e^\beta + (1)e^\beta + (-1)e^{-\beta} + (-1)e^{-\beta}
      \over
    e^\beta + e^\beta + e^{-\beta} +e^{-\beta}}
  \\ &=     {2e^\beta - 2e^{-\beta}
      \over
    2e^\beta + 2e^{-\beta}}
  \\ &= \tanh\beta
  \end{align*}
  
\item
  Let $u=w_1,\ldots,w_k=v$ be the single simple path between $u$ and $v$.
  \begin{align*}
    \E[\sigma_u\sigma_v] &=
    \E\left[\sigma_u\prod_{i=2}^{k-1}\sigma_{w_i}^2\sigma_v\right]
      \\ &= \E\left[\sigma_{w_1}\prod_{i=2}^{k-1}\sigma_{w_i}^2\sigma_{w_k}\right]
      \\ &= \E\left[\prod_{i=1}^{k-1}\sigma_{w_i}\sigma_{w_{i+1}}\right]
  \end{align*}
  Now, by the symmetry of the Ising model, conditioning on
  $\sigma_{w_i}\sigma_{w_{i+1}}$ does not change the marginal of
  $\sigma_{w_{i+1}}$, and so doesn't change the distribution of
  $\sigma_{w_{i+2}}$. We can therefore write:
  \begin{align*}
    \E[\sigma_u\sigma_v] &= \prod_{i=1}^{k-1}\E\left[\sigma_{w_i}\sigma_{w_{i+1}}\right]
  \end{align*}
  The marginal distribution of a couple of two connected vertices
  $u,v$ with coefficient $\beta_{u,v}$ is the same as that of an Ising
  model on two vertices with coefficient $\beta_{u,v}$. Hence
  \begin{align*}
    \E[\sigma_u\sigma_v] &= \prod_{i=1}^{k-1}\tanh\beta_{w_i,w_{i+1}}
  \end{align*}
  
\item Since $\beta_{u,v}>0$ then $0<tanh\beta_{u,v}<1$ and
  the weights $-\log\E[\sigma_u\sigma_v]$ are positive. Also,
  \begin{align*}
    \E[\sigma_u\sigma_v] &=
    \prod_{i=1}^{k-1}\E\left[\sigma_{w_i}\sigma_{w_{i+1}}\right]
  \end{align*}
  implies
  \begin{align*}
    -\log\E[\sigma_u\sigma_v] &=
    \sum_{i=1}^{k-1}-\log\E\left[\sigma_{w_i}\sigma_{w_{i+1}}\right]
  \end{align*}
  and so the distance between two vertices is the sum of the weights
  on the edges along the shortest path between them. Thus this defines
  a graph metric, which in particular is a metric.
\item Given an unbounded number of edges we can reconstruct
  $\beta_{u,v}$ exactly, with probability one. We saw in class how to
  reconstruct a tree $T$ given the distances between the nodes by some
  tree metric: the tree is the Minimum Spanning Tree.
\item The following algorithm reconstructs a tree with $n$ nodes:
  \begin{enumerate}
  \item Denote $\delta = \tanh(10^{-6})(1-\tanh(10^6))$. Draw
    $k=-2{\log\eps\over \delta^2}$ samples from the distribution.
  \item For each pair $u,v$, estimate $c_{u,v}=\E[\sigma_u\sigma_v]$
    by its sample average $\hat{c}_{u,v}$: denote by $l_{u,v}$ the
    number of samples for which $\sigma_u=\sigma_v$. Then
    $\hat{c}_{u,v}=(2l_{uv}-k)/k$. Calculate $\hat{w}_{u,v}=-\log
    \hat{c}_{u,v}$.
  \item Return an MST on $\hat{w}_{u,v}$. 
  \end{enumerate}
  We show that this algorithm returns the wrong answer with error
  probability at most $\eps$: Assume we run Kruskal's MST
  algorithm\footnote{Reminder: Kruskal's algorithm orders the edges by
    increasing distance, considers the edges according to this order
    and adds a considered edge to the solution iff it doesn't create a
    cycle}. Let $e'=(u',v')$ be the first edge added to the
  algorithm's answer $T'$ which is not in $T$. Then the ``real''
  shortest path between $u'$ and $v'$ is of length at least two, and
  furthermore there must exist an edge $e \in T$ that is on the
  shortest path between $u'$ and $v'$ and was not added yet.

  Since $e$ was not added yet then
  $\hat{c}_{e'}>\hat{c}_{e}$. However, since the length of the path is
  at least two, then
  $c_{e'}<c_{e}\tanh(10^6)<c_e-\tanh(10^{-6})(1-\tanh(10^6))=c_e-\delta$.  
  The probability for such an error is:
  \begin{align*}
    \P[\hat{c}_{e'}-c_{e'}>\delta] &= \P\left[(2l_{u,v}-k)/k-c_{e'}>\delta\right]
    \\ &= \P\left[l_{u,v}-\half(k+kc_{e'})>\half k\delta\right]
    \\ &= \P\left[l_{u,v}-\E[l_{u,v}]>\half k\delta\right]
    \\ &\leq exp(-\half k\delta^2)
    \\ = \eps
  \end{align*}
\end{itemize}

\end{document}


















