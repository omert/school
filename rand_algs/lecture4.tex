%&latex
\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\input{defs.tex}
\begin{document}

\lecture{4}{}{Moni Naor}{Tamar Zondiner \& Omer Tamuz}
\section{Application of pairwise independent hash functions}
In the previous lecture we encountered two families of pairwise
independent hash function: $H_{\mathrm{subset}}$ and
$H_{\mathrm{linear}}$. A similar family is $H_{\mathrm{matrix}}$: a
function $h_A \in H_{\mathrm{matrix}}$ is defined by $h_A(x) = Ax$,
where $x$ is a vector in $F^n$ (for some finite field $F$) and
$A$ is an $n$-by-$n$ matrix over $F$. Note that we must restrict $x$
to not equal zero.

Is it possible to construct a family of pairwise independent hash
functions that are all permutations? This is in fact easy: a function
$h_{a,b} \in H_{\mathrm{linear}}$ is a permutation iff $a \neq
0$. However, requiring $k$-wise independence for $k>3$ makes this
problem more difficult, and no elegant solution is currently known.

We can use families of pairwise independent hash functions for message
authentication. Consider two parties: Alice and Bob. Alice wants to
send a message to Bob, but Eve stands between them, and Eve might
tamper with the message. Alice and Bob would like to find an
authentication mechanism that would help detect tampering.

It can be shown that if Alice and Bob have no secrets from Eve, then
there's nothing they can do. Let's assume then that they share a
secret that is unknown to Eve. In particular, suppose this secret is a
hash function $h$, picked uniformly at random from some family of
pairwise independent hash functions.

To send an authenticated message $m$, Alice appends $h(m)$ to $m$. Eve
may change this message and send instead $m',s$, which would succeed
only if $s=h(m')$. The probability of success can be made arbitrarily
small, by choosing a large enough family of functions.

Note that this scheme cannot be used repeatedly. For example, a linear
hash function can be learned from two such authenticated messages.

\section{Multiset equality}
Alice and Bob each receive streams $\{z_i\}_{i=1}^l$ and
$\{w_i\}_{i=1}^l$, respectively. They would like to determine whether
$z$ and $w$ are equal as multisets. Formally, this holds if there
exists a permutation $\sigma$ such that $z_i=w_{\sigma(i)}$ for all
$i$. Na\"ively, they could send each other the entire stream, sort
each stream and compare them. We would like to find a solution that
has lower communication complexity.

A possible approach would be for them to calculate $\sum_{i=1}^l z_i$
and $\sum_{j=1}^l w_j$ and compare the result of this
calculation. However, in an adversarial model this would give no
non-trivial lower bound on the probability of declaring the streams
equal when in fact they are not.

A more fruitful approach is to consider the {\em product} of the
elements, rather than their sum. We regard each $w_i$ and $z_i$ as an
element of some finite field $F$. Then:
\begin{claim}
  Let $Z(x)=\prod_{i=1}^l(z_i-x)$ and $W(x)=\prod_{i=1}^l(w_i-x)$. Then, for a
  uniformly random choice of $x \in F$, it holds that
  \begin{align*}
    \Pr[Z(x)=W(x)] \leq \frac{l-1}{|F|}
  \end{align*}
  whenever $Z \neq W$.
\end{claim}
\begin{proof}
  The polynomial $P(x)=Z(x)-W(x)$ is of degree $l-1$, and therefore
  has at most $l-1$ roots. Hence $W(x)$ is equal to $Z(x)$ in at most
  $l-1$ values of $x$, out of the possible $|F|$ values.
\end{proof}

The proposed algorithm is therefore for Alice and Bob to calculate
$W(x)$ and $Z(x)$ for some common, random $x$. Clearly, if the streams
are equal then $W(x) = Z(x)$. By the claim above, when the streams are
not equal then equality can be made arbitrarily rare, by an
appropriate choice of $F$.


\subsection{Application to memory checking}

Consider a CPU which uses a large external memory store. The CPU
doesn't trust the store, as it is possibly unreliable and may change a
stored value adversarially. We would like to find a way for the CPU to
verify that indeed every value read from memory is the same as the
last value written there.

One may take two approaches here: an on-line and an off-line
approach. In the on-line approach the CPU verifies continuously that
the memory is not corrupt. In the off-line approach verification takes
place only after the termination of the program. We focus on the
off-line case.

To implement our algorithm, we add to each memory cell a timestamp
cell. Also, the CPU maintains a counter $t$, which it advances with
every memory access operation. We refer to $t$ as the CPU's ``current
time''. We assume that the interaction between the CPU and memory is
performed via two function calls: $(v,t) = \mathrm{read}(a)$ and
$\mathrm{write}(a,v,t)$. Here $a$ is the location of a memory cell,
$v$ is a value to be written, and $t$ is the timestamp to be written.

To verify the memory integrity, the CPU adds the following actions to
its executions:
\begin{enumerate}
\item Start with writing `0' in all locations. 
\item After each `read' perform a `write' with the same value and the
  current time stamp.
\item Before `write' - read location (i.e., don't overwrite without
  checking).
\item At the end read all locations.
\item Always check that the current time stamp is greater than the
  timestamp read.
\end{enumerate}
Hence the timestamp at each cell records the last time a value was
written to that cell. Therefore, if a value read at a cell has a
timestamp larger than the current time then clearly the memory is
corrupt. We assume henceforth that no such event is detected.

Let $R$ equal the set of triplets $(a,v,t)$ read at the time of
termination of the program, and likewise let $W$ equal the set of
triplets $(a,v,t)$ written up to that time.

Given the CPU's actions, the following claim is easy to verify:
\begin{claim}
  If no time-stamp read was higher than the current time-stamp, then
  $W=R$ if and only if the memory has functioned correctly.
\end{claim}

Storing and comparing $W$ and $R$ is obviously not an option in this
model. However, we can use our multiset equality algorithm to perform
this with constant space requirements as high chances of success. Our
sets $W$ and $R$ become the two streams $\{w_i\}$ and $\{z_i\}$, and
the polynomials $W(x)$ and $Z(x)$ can be calculated on the fly, after
$x$ is chosen at random at the beginning of the process. Then
$W(z) = Z(x)$ if the memory is not corrupt, and $W(z) \neq Z(x)$ with
high probability when it is.


\section{Multivariate polynomials}
Let $Q({x_1, \ldots, x_n})$ be an $n$-variate polynomial over some
finite field $F$. We define the degree of a term of $Q$,
$Cx_1^{e_1}x_2^{e_2} \cdots x_n^{e_n}$, to be $\sum_{e_i}$, and the
degree of $Q$ to be the maximum degree of any of its terms.

When given $Q$ implicitly, an interesting question is whether $Q$ is
identically zero. If we have the possibility of efficiently evaluating
it then a possible approach would be to evaluate it on a uniformly
random argument $r = (r_1, \ldots, r_n)$. If $Q = 0$ then the result
would definitely equal zero. We would like to calculate the
probability that $Q(r) = 0$ when $Q \neq 0$.

The following lemma was proven independently by
Scwartz~\cite{Schwartz:1980} and Zippel~\cite{Zippel:1979}, and
presented at the same conference, where Zippel spoke right after
Schwartz, according to the program. A slightly weaker result was
proved earlier by Demillo and
Lipton~\cite{DemilloLipton:1978}. See~\cite{LiptonBlog} for an
in-depth discussion of the mathematics, history and politics.

\begin{lemma}[Schwartz-Zippel]
  Let $r = (r_1, \ldots, r_n)$ be drawn from the uniform distribution
  over $S^n$, for some $S \subseteq F$. Then
  \begin{align*}
    \Pr[Q(r) = 0] \leq \frac{d}{|S|}.
  \end{align*}
  for any polynomial $Q \neq 0$ with degree $d$.
\end{lemma}
\begin{proof}
  We prove by induction on $n$. The base of the induction is the fact
  that a single variable polynomial of degree $d$ can have no more
  than $d$ roots.

  Assume that the theorem holds up to $n-1$. Let $k$ be the highest
  degree of $x_1$ in $Q$. Then there exist polynomials $Q_1$ and $Q_2$
  such that
  \begin{align*}
    Q(x_1, \ldots, x_n) = x_1^kQ_1(x_2, \ldots, x_n) + Q_2(x_1, \ldots, x_n).
  \end{align*}

  Let $A$ be the event that $Q(r) = 0$, and let $B$ be the event that
  $Q_1(r) = 0$. For any two events $A$ and $B$ it holds that $\Pr[A]
  \leq \Pr[B] + \Pr[A| \neg B]$. By the inductive hypothesis we know
  that $\Pr[B] \leq \frac{d-k}{|S|}$, so all we've left to do is bound
  $\Pr[A | \neg B]$.
  
  Now, for any fixed choice of $r_2, \ldots, r_n$ for which $Q_1 \neq
  0$, $Q_1$ is a constant and $Q_2$ is a single variable polynomial in
  $r_1$, and as such has degree less than $k$. Thus $Q$ is (after
  fixing $r_2, \ldots, r_n$) a single variable polynomial of degree
  $k$, and as such has at most $k$ zeros, and so the probability of
  $r$ being a zero is at most $k/|S|$. Since this is true conditioned on
  $r_2, \ldots, r_n$ it must also hold on average, and so $\Pr[A |
  \neg B] \leq \frac{k}{|S|}$.

  We have then that
  \begin{align*}
    \Pr[Q(r) = 0] &= \Pr[A]
    \\ &\leq \Pr[B] + \Pr[A| \neg B]
    \\ &\leq \frac{d-k}{|S|} + \frac{k}{|S|}
    \\ &= \frac{d}{|S|}
  \end{align*}
\end{proof}

Recently, Moshkovitz~\cite{Moshkovitz:2010} published a short
alternative proof for this lemma, for the case of $S=F$. For fixed $y
\in F^n$ and $x \in F^{n-1}$, $Q(x+ty)$ is a single variable
polynomial in $t$ of degree $d$, and as such has at most $d$ zeros, or
a probability of $d/|F|$ for a random $t$ to be a zero. The theorem
follows from the fact that $F^n$ is partitioned into lines by the
choice of $x$ and $y$.


\subsection{Application to perfect matchings and more}
Let $G=(U,V,E)$ be a bipartite graph, with $|U|=|V|=n$. A perfect
matching is a subset of $n$ edges such that each vertex touches a
unique edge.

Perfect matchings can be found with flow algorithms, which are
non-trivial. We will show a simple randomized solution, using
multivariate polynomial identity testing.

Let $M$ be the adjacency matrix of a $G$, so that for $i \in U$ and $j
\in V$ it holds that
\begin{align*}
M_{i,j}=
\begin{cases}
  1 & (i,j)\in E \\
  0 & \mathrm{otherwise} \\
\end{cases}.  
\end{align*}
The Tutte Matrix $T$ is constructed by replacing each non-zero element
$M_{ij}$ of $M$ with the formal variable $x_{ij}$:
\begin{align*}
T_{i,j}=
\begin{cases}
  x_{ij} & (i,j)\in E \\
  0 & \mathrm{otherwise} \\
\end{cases}.  
\end{align*}
It is easy to see that $\det(T)\neq 0$ iff $G$ has a perfect matching,
since the determinant is the sum of all permutations:
$\sum_{\pi}(-1)^{\mathrm{sgn} \pi}\prod_{i=1}^{n}T_{i,\pi(i)}$.  The algorithm
is no trivial: choose random assignments to $\{x_{ij}\}$ and calculate
$\det(T)$. Since $\det(T)$ is a polynomial of degree $n$, the
probability that it will be zero for a random assignment is at most
$n/|F|$, given that a perfect matching exists. We can control the
probability of error by choosing a large $F$.

An advantage of this algorithm over flow algorithms is that it is easy
to parallelize, whereas flow algorithms are inherently sequential. 


Primality testing can also be viewed as identity testing. Consider the
following simple algorithm for primality testing: given a number $n$,
define $P_n(z)=(1+z)^{n}-1-z^n$. Then $P_n(z)=0 \mod n \iff$ z is
prime, and thus testing for the primality of $n$ is equivalent to
testing whether $P_n(z)$ is identically zero.

\bibliographystyle{abbrv} \bibliography{lectures}

\end{document}

